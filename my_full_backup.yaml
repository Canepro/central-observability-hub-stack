apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-11-18T20:38:38Z"
    generateName: cert-manager-7d678bfb4f-
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.3
      pod-template-hash: 7d678bfb4f
    name: cert-manager-7d678bfb4f-8vfzg
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-7d678bfb4f
      uid: c50ea17a-07ca-4f38-9046-93561098b713
    resourceVersion: "1957980"
    uid: 393ff539-7995-4c54-ae56-5c4d19a35b0d
  spec:
    containers:
    - args:
      - --v=2
      - --cluster-resource-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=kube-system
      - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.3
      - --max-concurrent-challenges=60
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-controller:v1.13.3
      imagePullPolicy: IfNotPresent
      name: cert-manager-controller
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      - containerPort: 9403
        name: http-healthz
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2g6m9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: 10.0.10.209
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager
    serviceAccountName: cert-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-2g6m9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:47Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:38Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:47Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:38Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://60ce5e12d8653c692e849452dab9ffb0d65b7dfb919c8fe6f53da0a74bbb511e
      image: quay.io/jetstack/cert-manager-controller:v1.13.3
      imageID: quay.io/jetstack/cert-manager-controller@sha256:2121d4250f5734ee097df243507d06536fc264140dba3425045a825ef597c79d
      lastState: {}
      name: cert-manager-controller
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T20:38:46Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 1000
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2g6m9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.31
    podIPs:
    - ip: 10.0.40.31
    qosClass: BestEffort
    startTime: "2025-11-18T20:38:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-18T20:38:38Z"
    generateName: cert-manager-cainjector-7449dc67b9-
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.13.3
      pod-template-hash: 7449dc67b9
    name: cert-manager-cainjector-7449dc67b9-66mg4
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-cainjector-7449dc67b9
      uid: c511d8cc-c455-45aa-b2b3-97bdd5024bbb
    resourceVersion: "1957956"
    uid: 40b42029-548f-4027-8a7d-b16afd4ba526
  spec:
    containers:
    - args:
      - --v=2
      - --leader-election-namespace=kube-system
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-cainjector:v1.13.3
      imagePullPolicy: IfNotPresent
      name: cert-manager-cainjector
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4kdgh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: 10.0.10.209
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-cainjector
    serviceAccountName: cert-manager-cainjector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-4kdgh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:45Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:38Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:45Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:38Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://f92a730da59da8cee44ba52b446bed17c74b9b29e66229b9680fa1fb067886e2
      image: quay.io/jetstack/cert-manager-cainjector:v1.13.3
      imageID: quay.io/jetstack/cert-manager-cainjector@sha256:ac5154525f99bd0872671613741aac1b7dcb9c0df988571a7618155ddb6fabd2
      lastState: {}
      name: cert-manager-cainjector
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T20:38:44Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 1000
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4kdgh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.126
    podIPs:
    - ip: 10.0.40.126
    qosClass: BestEffort
    startTime: "2025-11-18T20:38:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-18T20:38:39Z"
    generateName: cert-manager-webhook-7789f864b7-
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.3
      pod-template-hash: 7789f864b7
    name: cert-manager-webhook-7789f864b7-dl9cp
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-webhook-7789f864b7
      uid: 5b849ad6-ad97-4cb9-9671-d41ddba768fa
    resourceVersion: "1958013"
    uid: 0094d817-e43e-410b-9975-5720c92fa9f3
  spec:
    containers:
    - args:
      - --v=2
      - --secure-port=10250
      - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
      - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
      - --dynamic-serving-dns-names=cert-manager-webhook
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-webhook:v1.13.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: cert-manager-webhook
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      - containerPort: 6080
        name: healthcheck
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zbmrj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: 10.0.10.7
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-webhook
    serviceAccountName: cert-manager-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-zbmrj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:45Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:39Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:52Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:52Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:38:39Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://4494b90a018856e85b3bda18b56a1622d0d3e9735366e3fc934dbc3e6db81d55
      image: quay.io/jetstack/cert-manager-webhook:v1.13.3
      imageID: quay.io/jetstack/cert-manager-webhook@sha256:afe9a27be1e6b3847d6483eb9a83b20fb8576ba5c314f381a90b185af022a105
      lastState: {}
      name: cert-manager-webhook
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T20:38:44Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 1000
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zbmrj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.77
    podIPs:
    - ip: 10.0.40.77
    qosClass: BestEffort
    startTime: "2025-11-18T20:38:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-18T20:18:16Z"
    generateName: ingress-nginx-controller-764b5b4897-
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
      pod-template-hash: 764b5b4897
    name: ingress-nginx-controller-764b5b4897-x2knk
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ingress-nginx-controller-764b5b4897
      uid: 33b8cd56-ad1c-438f-a5cb-fb6f30302a20
    resourceVersion: "1952148"
    uid: abdeb6f8-d54f-4ca4-bee6-0fc2845a477c
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --default-backend-service=$(POD_NAMESPACE)/ingress-nginx-defaultbackend
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --enable-metrics=true
      - --enable-ssl-passthrough
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.14.0@sha256:e4127065d0317bd11dc64c4dd38dcf7fb1c3d72e468110b4086e636dbaac943d
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      - containerPort: 10254
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w56kc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.7
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-w56kc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:18:29Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:18:16Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:18:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:18:40Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T20:18:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 128Mi
      containerID: cri-o://ae7b1365472c4a1045f94c04227233756c8a142d71b03856d8d02e00520e1d3c
      image: registry.k8s.io/ingress-nginx/controller@sha256:e4127065d0317bd11dc64c4dd38dcf7fb1c3d72e468110b4086e636dbaac943d
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:7f2b00bd369a972bfb09acfe8c2525b99caeeeb54ab71d2822343e8fd4222e27
      lastState: {}
      name: controller
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T20:18:28Z"
      user:
        linux:
          gid: 82
          supplementalGroups:
          - 82
          uid: 101
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w56kc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.49
    podIPs:
    - ip: 10.0.40.49
    qosClass: Burstable
    startTime: "2025-11-18T20:18:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-18T19:57:39Z"
    generateName: ingress-nginx-defaultbackend-6b98b5cfbb-
    generation: 1
    labels:
      app.kubernetes.io/component: default-backend
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
      pod-template-hash: 6b98b5cfbb
    name: ingress-nginx-defaultbackend-6b98b5cfbb-sbjzr
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ingress-nginx-defaultbackend-6b98b5cfbb
      uid: a088a9ef-aa0f-4c4b-88a4-75985269f0b5
    resourceVersion: "1946043"
    uid: 2dbde6df-3572-44fc-818a-47ffe2c52cdd
  spec:
    automountServiceAccountToken: true
    containers:
    - image: registry.k8s.io/defaultbackend-amd64:1.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: ingress-nginx-default-backend
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 50m
          memory: 50Mi
        requests:
          cpu: 10m
          memory: 20Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrr6z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.209
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx-backend
    serviceAccountName: ingress-nginx-backend
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-mrr6z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T19:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T19:57:39Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T19:57:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T19:57:45Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T19:57:39Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 10m
        memory: 20Mi
      containerID: cri-o://c38e108e19783818ef0883ca8ed8a5a6200e62e30eab59a95cd5e992dd6e6166
      image: registry.k8s.io/defaultbackend-amd64:1.5
      imageID: registry.k8s.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7
      lastState: {}
      name: ingress-nginx-default-backend
      ready: true
      resources:
        limits:
          cpu: 50m
          memory: 50Mi
        requests:
          cpu: 10m
          memory: 20Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T19:57:44Z"
      user:
        linux:
          gid: 65534
          supplementalGroups:
          - 65534
          uid: 65534
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrr6z
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.66
    podIPs:
    - ip: 10.0.40.66
    qosClass: Burstable
    startTime: "2025-11-18T19:57:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      version_hash: "1960218013"
    creationTimestamp: "2025-11-14T12:59:34Z"
    generateName: coredns-98c69cbf4-
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 98c69cbf4
    name: coredns-98c69cbf4-875vf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-98c69cbf4
      uid: f28369b1-5432-4238-87b5-18a83d0bb2f3
    resourceVersion: "259159"
    uid: fe351ad3-84a9-4b61-b078-d6c9634dc6a1
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: failure-domain.beta.kubernetes.io/zone
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:e32e8482ef16dbfd86896ece95e81111d5cb110811a65c3ce85df0ce2b69ca17
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d5rw9
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: 10.0.10.209
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - key: oci.oraclecloud.com/oke-is-preemptible
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-d5rw9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:44Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:35Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:44Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:35Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 70Mi
      containerID: cri-o://5bd99caebc18db4517f5fe1412fb57592673e3917a28bea1b278bbb4c91c9a4b
      image: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:e32e8482ef16dbfd86896ece95e81111d5cb110811a65c3ce85df0ce2b69ca17
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:610b60040c740a8e58a1528a8b2e82e631358c223deb9d8eeae7d4db9e7d4c8b
      lastState: {}
      name: coredns
      ready: true
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:43Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d5rw9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.102
    podIPs:
    - ip: 10.0.40.102
    qosClass: Burstable
    startTime: "2025-11-14T12:59:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      version_hash: "1960218013"
    creationTimestamp: "2025-11-14T12:59:35Z"
    generateName: coredns-98c69cbf4-
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 98c69cbf4
    name: coredns-98c69cbf4-bl74s
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-98c69cbf4
      uid: f28369b1-5432-4238-87b5-18a83d0bb2f3
    resourceVersion: "259204"
    uid: 47582389-eef5-424f-865f-a8f49db0c65a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: failure-domain.beta.kubernetes.io/zone
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:e32e8482ef16dbfd86896ece95e81111d5cb110811a65c3ce85df0ce2b69ca17
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wlblt
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: 10.0.10.83
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - key: oci.oraclecloud.com/oke-is-preemptible
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-wlblt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:52Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:41Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:53Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:35Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 70Mi
      containerID: cri-o://d6b08c8d1462ed59bf980457285bbd089b17cfff4de850c8e8b4199871cb3070
      image: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:e32e8482ef16dbfd86896ece95e81111d5cb110811a65c3ce85df0ce2b69ca17
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:610b60040c740a8e58a1528a8b2e82e631358c223deb9d8eeae7d4db9e7d4c8b
      lastState: {}
      name: coredns
      ready: true
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:52Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wlblt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.10
    podIPs:
    - ip: 10.0.40.10
    qosClass: Burstable
    startTime: "2025-11-14T12:59:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      version_hash: "1960218013"
    creationTimestamp: "2025-11-13T19:53:34Z"
    generateName: coredns-98c69cbf4-
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 98c69cbf4
    name: coredns-98c69cbf4-sgmmq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-98c69cbf4
      uid: f28369b1-5432-4238-87b5-18a83d0bb2f3
    resourceVersion: "258999"
    uid: feafa18b-9a1a-4763-b41e-b1d2f4fcd8f0
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: failure-domain.beta.kubernetes.io/zone
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:e32e8482ef16dbfd86896ece95e81111d5cb110811a65c3ce85df0ce2b69ca17
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6nct6
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: 10.0.10.7
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - key: oci.oraclecloud.com/oke-is-preemptible
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-6nct6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:20Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:07Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:20Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:20Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:07Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 70Mi
      containerID: cri-o://bdceeddc5896988a30e6cba56853a4788884b1628e96f3a05b0b639022a28c15
      image: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:e32e8482ef16dbfd86896ece95e81111d5cb110811a65c3ce85df0ce2b69ca17
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:610b60040c740a8e58a1528a8b2e82e631358c223deb9d8eeae7d4db9e7d4c8b
      lastState: {}
      name: coredns
      ready: true
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:19Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6nct6
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.108
    podIPs:
    - ip: 10.0.40.108
    qosClass: Burstable
    startTime: "2025-11-14T12:59:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 6bc5cc92b481f982
    creationTimestamp: "2025-11-14T12:57:51Z"
    generateName: csi-oci-node-
    generation: 1
    labels:
      app: csi-oci-node
      controller-revision-hash: 6f78b54594
      pod-template-generation: "1"
      role: csi-oci
    name: csi-oci-node-jrvgx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-oci-node
      uid: 4823b240-34b4-4a27-a053-fbc54c80210a
    resourceVersion: "258902"
    uid: daf0e989-ba4d-4d25-bc28-289703fe23a4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.83
    containers:
    - args:
      - --v=2
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/blockvolume.csi.oraclecloud.com/csi.sock
      - --endpoint=unix:///csi/csi.sock
      - --nodeid=$(KUBE_NODE_NAME)
      - --fss-endpoint=unix:///fss/csi.sock
      - --fss-csi-address=/fss/csi.sock
      - --fss-kubelet-registration-path=/var/lib/kubelet/plugins/fss.csi.oraclecloud.com/csi.sock
      - --fss-csi-driver-enabled=true
      - --lustre-endpoint=unix:///lustre/csi.sock
      - --lustre-csi-address=/lustre/csi.sock
      - --lustre-kubelet-registration-path=/var/lib/kubelet/plugins/lustre.csi.oraclecloud.com/csi.sock
      command:
      - /usr/local/bin/oci-csi-node-driver
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: PATH
        value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      - name: LUSTRE_DRIVER_ENABLED
        value: "true"
      image: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci:v1.34-8f0fbf7e71e-9-csi@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/blockvolume.csi.oraclecloud.com /registration/blockvolume.csi.oraclecloud.com-reg.sock
              /registration/fss.csi.oraclecloud.com /registration/fss.csi.oraclecloud.com-reg.sock
              /registration/lustre.csi.oraclecloud.com /registration/lustre.csi.oraclecloud.com-reg.sock
      name: csi-node-driver
      resources:
        limits:
          cpu: 500m
          memory: 300Mi
        requests:
          cpu: 30m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
      - mountPath: /sbin/iscsiadm
        name: chroot-iscsiadm
        subPath: iscsiadm
      - mountPath: /fss
        name: fss-plugin-dir
      - mountPath: /host/var/lib/kubelet
        mountPropagation: Bidirectional
        name: encrypt-pods-mount-dir
      - mountPath: /sbin/umount.oci-fss
        name: fss-driver-mounts
        subPath: umount.oci-fss
      - mountPath: /sbin/umount
        name: fss-driver-mounts
        subPath: umount
      - mountPath: /sbin/mount
        name: fss-driver-mounts
        subPath: mount
      - mountPath: /lustre
        name: lustre-plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mk8q8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: 10.0.10.83
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: csi-oci-node-sa
    serviceAccountName: csi-oci-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/blockvolume.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - configMap:
        defaultMode: 493
        items:
        - key: iscsiadm
          path: iscsiadm
        name: oci-csi-iscsiadm
      name: chroot-iscsiadm
    - hostPath:
        path: /var/lib/kubelet/plugins/fss.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: fss-plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: encrypt-pods-mount-dir
    - configMap:
        defaultMode: 493
        name: oci-fss-csi
      name: fss-driver-mounts
    - hostPath:
        path: /var/lib/kubelet/plugins/lustre.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: lustre-plugin-dir
    - name: kube-api-access-mk8q8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:10Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:52Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:10Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:10Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:51Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 30m
        memory: 70Mi
      containerID: cri-o://4e6a687ddacc8f9c83d22d2b8c8e9fe8562ebc8899a256a7350a513212813086
      image: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      imageID: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      lastState: {}
      name: csi-node-driver
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 300Mi
        requests:
          cpu: 30m
          memory: 70Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:09Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /host
        name: host-root
      - mountPath: /sbin/iscsiadm
        name: chroot-iscsiadm
      - mountPath: /fss
        name: fss-plugin-dir
      - mountPath: /host/var/lib/kubelet
        name: encrypt-pods-mount-dir
      - mountPath: /sbin/umount.oci-fss
        name: fss-driver-mounts
      - mountPath: /sbin/umount
        name: fss-driver-mounts
      - mountPath: /sbin/mount
        name: fss-driver-mounts
      - mountPath: /lustre
        name: lustre-plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mk8q8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.83
    podIPs:
    - ip: 10.0.10.83
    qosClass: Burstable
    startTime: "2025-11-14T12:57:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 6bc5cc92b481f982
    creationTimestamp: "2025-11-14T12:57:45Z"
    generateName: csi-oci-node-
    generation: 1
    labels:
      app: csi-oci-node
      controller-revision-hash: 6f78b54594
      pod-template-generation: "1"
      role: csi-oci
    name: csi-oci-node-rh4cf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-oci-node
      uid: 4823b240-34b4-4a27-a053-fbc54c80210a
    resourceVersion: "258797"
    uid: e169d41f-2643-4ef0-941d-38d2d0503a25
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.209
    containers:
    - args:
      - --v=2
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/blockvolume.csi.oraclecloud.com/csi.sock
      - --endpoint=unix:///csi/csi.sock
      - --nodeid=$(KUBE_NODE_NAME)
      - --fss-endpoint=unix:///fss/csi.sock
      - --fss-csi-address=/fss/csi.sock
      - --fss-kubelet-registration-path=/var/lib/kubelet/plugins/fss.csi.oraclecloud.com/csi.sock
      - --fss-csi-driver-enabled=true
      - --lustre-endpoint=unix:///lustre/csi.sock
      - --lustre-csi-address=/lustre/csi.sock
      - --lustre-kubelet-registration-path=/var/lib/kubelet/plugins/lustre.csi.oraclecloud.com/csi.sock
      command:
      - /usr/local/bin/oci-csi-node-driver
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: PATH
        value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      - name: LUSTRE_DRIVER_ENABLED
        value: "true"
      image: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci:v1.34-8f0fbf7e71e-9-csi@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/blockvolume.csi.oraclecloud.com /registration/blockvolume.csi.oraclecloud.com-reg.sock
              /registration/fss.csi.oraclecloud.com /registration/fss.csi.oraclecloud.com-reg.sock
              /registration/lustre.csi.oraclecloud.com /registration/lustre.csi.oraclecloud.com-reg.sock
      name: csi-node-driver
      resources:
        limits:
          cpu: 500m
          memory: 300Mi
        requests:
          cpu: 30m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
      - mountPath: /sbin/iscsiadm
        name: chroot-iscsiadm
        subPath: iscsiadm
      - mountPath: /fss
        name: fss-plugin-dir
      - mountPath: /host/var/lib/kubelet
        mountPropagation: Bidirectional
        name: encrypt-pods-mount-dir
      - mountPath: /sbin/umount.oci-fss
        name: fss-driver-mounts
        subPath: umount.oci-fss
      - mountPath: /sbin/umount
        name: fss-driver-mounts
        subPath: umount
      - mountPath: /sbin/mount
        name: fss-driver-mounts
        subPath: mount
      - mountPath: /lustre
        name: lustre-plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w49m8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: 10.0.10.209
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: csi-oci-node-sa
    serviceAccountName: csi-oci-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/blockvolume.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - configMap:
        defaultMode: 493
        items:
        - key: iscsiadm
          path: iscsiadm
        name: oci-csi-iscsiadm
      name: chroot-iscsiadm
    - hostPath:
        path: /var/lib/kubelet/plugins/fss.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: fss-plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: encrypt-pods-mount-dir
    - configMap:
        defaultMode: 493
        name: oci-fss-csi
      name: fss-driver-mounts
    - hostPath:
        path: /var/lib/kubelet/plugins/lustre.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: lustre-plugin-dir
    - name: kube-api-access-w49m8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:46Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 30m
        memory: 70Mi
      containerID: cri-o://ac1cb6032d48d2b598fee9270aa0d6493e4a3692c66c6fde741bf20f4e3e2f0c
      image: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      imageID: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      lastState: {}
      name: csi-node-driver
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 300Mi
        requests:
          cpu: 30m
          memory: 70Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:00Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /host
        name: host-root
      - mountPath: /sbin/iscsiadm
        name: chroot-iscsiadm
      - mountPath: /fss
        name: fss-plugin-dir
      - mountPath: /host/var/lib/kubelet
        name: encrypt-pods-mount-dir
      - mountPath: /sbin/umount.oci-fss
        name: fss-driver-mounts
      - mountPath: /sbin/umount
        name: fss-driver-mounts
      - mountPath: /sbin/mount
        name: fss-driver-mounts
      - mountPath: /lustre
        name: lustre-plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w49m8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.209
    podIPs:
    - ip: 10.0.10.209
    qosClass: Burstable
    startTime: "2025-11-14T12:57:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 6bc5cc92b481f982
    creationTimestamp: "2025-11-14T12:57:45Z"
    generateName: csi-oci-node-
    generation: 1
    labels:
      app: csi-oci-node
      controller-revision-hash: 6f78b54594
      pod-template-generation: "1"
      role: csi-oci
    name: csi-oci-node-wfcqq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-oci-node
      uid: 4823b240-34b4-4a27-a053-fbc54c80210a
    resourceVersion: "258775"
    uid: d8d2f62b-a686-4d8c-95a5-fd62907e2991
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.7
    containers:
    - args:
      - --v=2
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/blockvolume.csi.oraclecloud.com/csi.sock
      - --endpoint=unix:///csi/csi.sock
      - --nodeid=$(KUBE_NODE_NAME)
      - --fss-endpoint=unix:///fss/csi.sock
      - --fss-csi-address=/fss/csi.sock
      - --fss-kubelet-registration-path=/var/lib/kubelet/plugins/fss.csi.oraclecloud.com/csi.sock
      - --fss-csi-driver-enabled=true
      - --lustre-endpoint=unix:///lustre/csi.sock
      - --lustre-csi-address=/lustre/csi.sock
      - --lustre-kubelet-registration-path=/var/lib/kubelet/plugins/lustre.csi.oraclecloud.com/csi.sock
      command:
      - /usr/local/bin/oci-csi-node-driver
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: PATH
        value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      - name: LUSTRE_DRIVER_ENABLED
        value: "true"
      image: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci:v1.34-8f0fbf7e71e-9-csi@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/blockvolume.csi.oraclecloud.com /registration/blockvolume.csi.oraclecloud.com-reg.sock
              /registration/fss.csi.oraclecloud.com /registration/fss.csi.oraclecloud.com-reg.sock
              /registration/lustre.csi.oraclecloud.com /registration/lustre.csi.oraclecloud.com-reg.sock
      name: csi-node-driver
      resources:
        limits:
          cpu: 500m
          memory: 300Mi
        requests:
          cpu: 30m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
      - mountPath: /sbin/iscsiadm
        name: chroot-iscsiadm
        subPath: iscsiadm
      - mountPath: /fss
        name: fss-plugin-dir
      - mountPath: /host/var/lib/kubelet
        mountPropagation: Bidirectional
        name: encrypt-pods-mount-dir
      - mountPath: /sbin/umount.oci-fss
        name: fss-driver-mounts
        subPath: umount.oci-fss
      - mountPath: /sbin/umount
        name: fss-driver-mounts
        subPath: umount
      - mountPath: /sbin/mount
        name: fss-driver-mounts
        subPath: mount
      - mountPath: /lustre
        name: lustre-plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5w2cs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: 10.0.10.7
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: csi-oci-node-sa
    serviceAccountName: csi-oci-node-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/blockvolume.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: device-dir
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - configMap:
        defaultMode: 493
        items:
        - key: iscsiadm
          path: iscsiadm
        name: oci-csi-iscsiadm
      name: chroot-iscsiadm
    - hostPath:
        path: /var/lib/kubelet/plugins/fss.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: fss-plugin-dir
    - hostPath:
        path: /var/lib/kubelet
        type: Directory
      name: encrypt-pods-mount-dir
    - configMap:
        defaultMode: 493
        name: oci-fss-csi
      name: fss-driver-mounts
    - hostPath:
        path: /var/lib/kubelet/plugins/lustre.csi.oraclecloud.com
        type: DirectoryOrCreate
      name: lustre-plugin-dir
    - name: kube-api-access-5w2cs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:00Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:46Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:00Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:00Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 30m
        memory: 70Mi
      containerID: cri-o://6a489a9ae20568ce4f7948f779268df5963c91294f070e96226b737dfb37df81
      image: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      imageID: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
      lastState: {}
      name: csi-node-driver
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 300Mi
        requests:
          cpu: 30m
          memory: 70Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:58:59Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet
        name: pods-mount-dir
      - mountPath: /dev
        name: device-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /host
        name: host-root
      - mountPath: /sbin/iscsiadm
        name: chroot-iscsiadm
      - mountPath: /fss
        name: fss-plugin-dir
      - mountPath: /host/var/lib/kubelet
        name: encrypt-pods-mount-dir
      - mountPath: /sbin/umount.oci-fss
        name: fss-driver-mounts
      - mountPath: /sbin/umount
        name: fss-driver-mounts
      - mountPath: /sbin/mount
        name: fss-driver-mounts
      - mountPath: /lustre
        name: lustre-plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5w2cs
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.7
    podIPs:
    - ip: 10.0.10.7
    qosClass: Burstable
    startTime: "2025-11-14T12:57:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      version_hash: "1960218013"
    creationTimestamp: "2025-11-13T19:53:36Z"
    generateName: kube-dns-autoscaler-6d5986cc55-
    generation: 1
    labels:
      k8s-app: kube-dns-autoscaler
      pod-template-hash: 6d5986cc55
    name: kube-dns-autoscaler-6d5986cc55-xz4cs
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-autoscaler-6d5986cc55
      uid: a5b0d573-e9e8-4ccd-8082-58f36304b824
    resourceVersion: "259102"
    uid: 4b6a4d66-6b8d-453b-a104-7242d6371f95
  spec:
    containers:
    - command:
      - /cluster-proportional-autoscaler
      - --namespace=kube-system
      - --configmap=kube-dns-autoscaler
      - --target=deployment/coredns
      - --logtostderr=true
      - --v=2
      image: iad.ocir.io/id9y6mi8tcky/oke-public-cluster-proportional-autoscaler-amd64@sha256:1908914e0c9055edd754a633de2a37fd6811a64565317f2f44bf4adea85f0654
      imagePullPolicy: IfNotPresent
      name: autoscaler
      resources:
        requests:
          cpu: 20m
          memory: 10Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j5j98
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.7
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dns-autoscaler
    serviceAccountName: dns-autoscaler
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: oci.oraclecloud.com/oke-is-preemptible
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-j5j98
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:35Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:07Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:35Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:07Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 20m
        memory: 10Mi
      containerID: cri-o://a6c595c0714e052f203d2dfae76403c240f27463898beddca34923c58f263473
      image: iad.ocir.io/id9y6mi8tcky/oke-public-cluster-proportional-autoscaler-amd64@sha256:1908914e0c9055edd754a633de2a37fd6811a64565317f2f44bf4adea85f0654
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-cluster-proportional-autoscaler-amd64@sha256:1908914e0c9055edd754a633de2a37fd6811a64565317f2f44bf4adea85f0654
      lastState: {}
      name: autoscaler
      ready: true
      resources:
        requests:
          cpu: 20m
          memory: 10Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:34Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j5j98
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.113
    podIPs:
    - ip: 10.0.40.113
    qosClass: Burstable
    startTime: "2025-11-14T12:59:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 6f554ff8e6a89ae0
      version_hash: "-1572170703"
    creationTimestamp: "2025-11-14T12:57:51Z"
    generateName: kube-proxy-
    generation: 1
    labels:
      controller-revision-hash: 8586dfb77f
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-29665
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: a453064c-b176-4fe6-a911-6f5a5db99786
    resourceVersion: "258934"
    uid: e84ed4a8-5c43-4a16-8449-6606502716a9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.83
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: PATH
        value: /hostIptables:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
        subPath: iptables
      - mountPath: /hostIptables/iptables-save
        name: chroot-iptables
        subPath: iptables-save
      - mountPath: /hostIptables/iptables-restore
        name: chroot-iptables
        subPath: iptables-restore
      - mountPath: /hostIptables/ip6tables
        name: chroot-iptables
        subPath: ip6tables
      - mountPath: /hostIptables/ip6tables-save
        name: chroot-iptables
        subPath: ip6tables-save
      - mountPath: /hostIptables/ip6tables-restore
        name: chroot-iptables
        subPath: ip6tables-restore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wsm7f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - /var/lib/kube-proxy-config/kube_proxy_init.sh
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imagePullPolicy: IfNotPresent
      name: init-kube-proxy
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /var/lib/kube-proxy-config
        name: kube-proxy-config-volume
      - mountPath: /var/lib/kube-proxy-k8s-version
        name: kube-proxy-k8s-version-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wsm7f
        readOnly: true
    nodeName: 10.0.10.83
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir: {}
      name: kube-proxy
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy-config-volume
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/oke/oke-k8s-version
        type: ""
      name: kube-proxy-k8s-version-volume
    - configMap:
        defaultMode: 493
        items:
        - key: iptables
          path: iptables
        - key: iptables-save
          path: iptables-save
        - key: iptables-restore
          path: iptables-restore
        - key: ip6tables
          path: ip6tables
        - key: ip6tables-save
          path: ip6tables-save
        - key: ip6tables-restore
          path: ip6tables-restore
        name: oci-iptables-kubeproxy
      name: chroot-iptables
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - name: kube-api-access-wsm7f
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:11Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:11Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:12Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:12Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:51Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://0ce7f6632fa67c73f2cd61ba9aee2d74e0d63230b7ce75e76791f54ab3154d56
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      lastState: {}
      name: kube-proxy
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:12Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
      - mountPath: /hostIptables/iptables-save
        name: chroot-iptables
      - mountPath: /hostIptables/iptables-restore
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables-save
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables-restore
        name: chroot-iptables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wsm7f
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    initContainerStatuses:
    - containerID: cri-o://7481d2cdf9e3b5328bb8e5a1c9e08b4f9a429342ec202ae4ddf68874d110d28b
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      lastState: {}
      name: init-kube-proxy
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://7481d2cdf9e3b5328bb8e5a1c9e08b4f9a429342ec202ae4ddf68874d110d28b
          exitCode: 0
          finishedAt: "2025-11-14T12:59:11Z"
          reason: Completed
          startedAt: "2025-11-14T12:59:11Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /var/lib/kube-proxy-config
        name: kube-proxy-config-volume
      - mountPath: /var/lib/kube-proxy-k8s-version
        name: kube-proxy-k8s-version-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wsm7f
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.83
    podIPs:
    - ip: 10.0.10.83
    qosClass: BestEffort
    startTime: "2025-11-14T12:57:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 6f554ff8e6a89ae0
      version_hash: "-1572170703"
    creationTimestamp: "2025-11-14T12:57:45Z"
    generateName: kube-proxy-
    generation: 1
    labels:
      controller-revision-hash: 8586dfb77f
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-2j7qs
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: a453064c-b176-4fe6-a911-6f5a5db99786
    resourceVersion: "258828"
    uid: 702728e2-cd8e-4b80-bfd7-348d7cafc2b6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.209
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: PATH
        value: /hostIptables:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
        subPath: iptables
      - mountPath: /hostIptables/iptables-save
        name: chroot-iptables
        subPath: iptables-save
      - mountPath: /hostIptables/iptables-restore
        name: chroot-iptables
        subPath: iptables-restore
      - mountPath: /hostIptables/ip6tables
        name: chroot-iptables
        subPath: ip6tables
      - mountPath: /hostIptables/ip6tables-save
        name: chroot-iptables
        subPath: ip6tables-save
      - mountPath: /hostIptables/ip6tables-restore
        name: chroot-iptables
        subPath: ip6tables-restore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvfk9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - /var/lib/kube-proxy-config/kube_proxy_init.sh
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imagePullPolicy: IfNotPresent
      name: init-kube-proxy
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /var/lib/kube-proxy-config
        name: kube-proxy-config-volume
      - mountPath: /var/lib/kube-proxy-k8s-version
        name: kube-proxy-k8s-version-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvfk9
        readOnly: true
    nodeName: 10.0.10.209
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir: {}
      name: kube-proxy
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy-config-volume
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/oke/oke-k8s-version
        type: ""
      name: kube-proxy-k8s-version-volume
    - configMap:
        defaultMode: 493
        items:
        - key: iptables
          path: iptables
        - key: iptables-save
          path: iptables-save
        - key: iptables-restore
          path: iptables-restore
        - key: ip6tables
          path: ip6tables
        - key: ip6tables-save
          path: ip6tables-save
        - key: ip6tables-restore
          path: ip6tables-restore
        name: oci-iptables-kubeproxy
      name: chroot-iptables
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - name: kube-api-access-xvfk9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:02Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:04Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://474109451e2ab5a0b0200773e8b34d7a1746d206ca24d6281252cc2595a181a7
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      lastState: {}
      name: kube-proxy
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:03Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
      - mountPath: /hostIptables/iptables-save
        name: chroot-iptables
      - mountPath: /hostIptables/iptables-restore
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables-save
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables-restore
        name: chroot-iptables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvfk9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    initContainerStatuses:
    - containerID: cri-o://a6eaa40a63b1f93a3d9d8be28bc548a48afc774aa15f6de5b34ae7d592d31269
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      lastState: {}
      name: init-kube-proxy
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://a6eaa40a63b1f93a3d9d8be28bc548a48afc774aa15f6de5b34ae7d592d31269
          exitCode: 0
          finishedAt: "2025-11-14T12:59:01Z"
          reason: Completed
          startedAt: "2025-11-14T12:59:01Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /var/lib/kube-proxy-config
        name: kube-proxy-config-volume
      - mountPath: /var/lib/kube-proxy-k8s-version
        name: kube-proxy-k8s-version-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xvfk9
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.209
    podIPs:
    - ip: 10.0.10.209
    qosClass: BestEffort
    startTime: "2025-11-14T12:57:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 6f554ff8e6a89ae0
      version_hash: "-1572170703"
    creationTimestamp: "2025-11-14T12:57:45Z"
    generateName: kube-proxy-
    generation: 1
    labels:
      controller-revision-hash: 8586dfb77f
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-d85gd
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: a453064c-b176-4fe6-a911-6f5a5db99786
    resourceVersion: "258805"
    uid: d5c16880-3fbc-4d3c-8830-61652e303a34
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.7
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: PATH
        value: /hostIptables:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
        subPath: iptables
      - mountPath: /hostIptables/iptables-save
        name: chroot-iptables
        subPath: iptables-save
      - mountPath: /hostIptables/iptables-restore
        name: chroot-iptables
        subPath: iptables-restore
      - mountPath: /hostIptables/ip6tables
        name: chroot-iptables
        subPath: ip6tables
      - mountPath: /hostIptables/ip6tables-save
        name: chroot-iptables
        subPath: ip6tables-save
      - mountPath: /hostIptables/ip6tables-restore
        name: chroot-iptables
        subPath: ip6tables-restore
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ck46g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - /var/lib/kube-proxy-config/kube_proxy_init.sh
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imagePullPolicy: IfNotPresent
      name: init-kube-proxy
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /var/lib/kube-proxy-config
        name: kube-proxy-config-volume
      - mountPath: /var/lib/kube-proxy-k8s-version
        name: kube-proxy-k8s-version-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ck46g
        readOnly: true
    nodeName: 10.0.10.7
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - emptyDir: {}
      name: kube-proxy
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy-config-volume
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/oke/oke-k8s-version
        type: ""
      name: kube-proxy-k8s-version-volume
    - configMap:
        defaultMode: 493
        items:
        - key: iptables
          path: iptables
        - key: iptables-save
          path: iptables-save
        - key: iptables-restore
          path: iptables-restore
        - key: ip6tables
          path: ip6tables
        - key: ip6tables-save
          path: ip6tables-save
        - key: ip6tables-restore
          path: ip6tables-restore
        name: oci-iptables-kubeproxy
      name: chroot-iptables
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - name: kube-api-access-ck46g
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:00Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:02Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://380a9cd5a5369bf57ab1a02afdd0ef8cda2e7aa58b6bf14daaa25d4bd7b37376
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      lastState: {}
      name: kube-proxy
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:02Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
      - mountPath: /hostIptables/iptables-save
        name: chroot-iptables
      - mountPath: /hostIptables/iptables-restore
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables-save
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables-restore
        name: chroot-iptables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ck46g
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    initContainerStatuses:
    - containerID: cri-o://d702bead5c5cb86ea7414b0a73cee81526eab6bbe591e186b9169ecb8d658885
      image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
      lastState: {}
      name: init-kube-proxy
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://d702bead5c5cb86ea7414b0a73cee81526eab6bbe591e186b9169ecb8d658885
          exitCode: 0
          finishedAt: "2025-11-14T12:59:00Z"
          reason: Completed
          startedAt: "2025-11-14T12:58:59Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /var/lib/kube-proxy-config
        name: kube-proxy-config-volume
      - mountPath: /var/lib/kube-proxy-k8s-version
        name: kube-proxy-k8s-version-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ck46g
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.7
    podIPs:
    - ip: 10.0.10.7
    qosClass: BestEffort
    startTime: "2025-11-14T12:57:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-14T12:57:51Z"
    generateName: proxymux-client-
    generation: 1
    labels:
      controller-revision-hash: 59d4c87876
      oke-app: proxymux-client-ds
      pod-template-generation: "1"
    name: proxymux-client-b9n5z
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: proxymux-client
      uid: cb2f39d0-7c13-4116-a1f3-9d3b3ae36581
    resourceVersion: "258921"
    uid: 1b549aa6-7695-4411-85bb-3d599c48db0e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.83
    containers:
    - args:
      - --config=/mnt/etc/proxymux/config.yaml
      - --verbosity=info
      image: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli:8d4509c1e518fcb4e1a95191a4b6dff29a283dee-115@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      imagePullPolicy: IfNotPresent
      name: proxymux-client
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/etc/proxymux/
        name: proxymux-cfg
      - mountPath: /etc/kubernetes/
        name: kubernetes-cfg
      - mountPath: /run/systemd/
        name: systemd
      - mountPath: /var/run/dbus/
        name: dbus
      - mountPath: /etc/pki/
        name: pki
      - mountPath: /etc/kubernetes/kube-root-ca/
        name: kube-root-ca
      - mountPath: /etc/kubernetes/kube-certificate-rotation/
        name: kube-certificate-rotation
      - mountPath: /var/lib/kubelet/pki
        name: var-lib-kubelet-pki
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pj5kk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: 10.0.10.83
    nodeSelector:
      node.info.ds_proxymux_client: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: proxymux-client
    serviceAccountName: proxymux-client
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/proxymux/
        type: ""
      name: proxymux-cfg
    - hostPath:
        path: /etc/kubernetes/
        type: ""
      name: kubernetes-cfg
    - hostPath:
        path: /run/systemd/
        type: ""
      name: systemd
    - hostPath:
        path: /var/run/dbus/
        type: ""
      name: dbus
    - hostPath:
        path: /etc/pki/
        type: ""
      name: pki
    - configMap:
        defaultMode: 420
        name: kube-root-ca.crt
        optional: true
      name: kube-root-ca
    - configMap:
        defaultMode: 420
        name: kube-certificate-rotation
        optional: true
      name: kube-certificate-rotation
    - hostPath:
        path: /var/lib/kubelet/pki/
        type: ""
      name: var-lib-kubelet-pki
    - name: kube-api-access-pj5kk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:11Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:52Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:11Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:11Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:51Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 64Mi
      containerID: cri-o://20b3b48d138184a3baae85099b666482d36b07982ae04fed605d05e40979a354
      image: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      imageID: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      lastState: {}
      name: proxymux-client
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:10Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /mnt/etc/proxymux/
        name: proxymux-cfg
      - mountPath: /etc/kubernetes/
        name: kubernetes-cfg
      - mountPath: /run/systemd/
        name: systemd
      - mountPath: /var/run/dbus/
        name: dbus
      - mountPath: /etc/pki/
        name: pki
      - mountPath: /etc/kubernetes/kube-root-ca/
        name: kube-root-ca
      - mountPath: /etc/kubernetes/kube-certificate-rotation/
        name: kube-certificate-rotation
      - mountPath: /var/lib/kubelet/pki
        name: var-lib-kubelet-pki
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pj5kk
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.83
    podIPs:
    - ip: 10.0.10.83
    qosClass: Burstable
    startTime: "2025-11-14T12:57:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-14T12:57:45Z"
    generateName: proxymux-client-
    generation: 1
    labels:
      controller-revision-hash: 59d4c87876
      oke-app: proxymux-client-ds
      pod-template-generation: "1"
    name: proxymux-client-q5kql
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: proxymux-client
      uid: cb2f39d0-7c13-4116-a1f3-9d3b3ae36581
    resourceVersion: "258794"
    uid: 3e0bed90-450b-4e53-b34a-21771a52c820
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.209
    containers:
    - args:
      - --config=/mnt/etc/proxymux/config.yaml
      - --verbosity=info
      image: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli:8d4509c1e518fcb4e1a95191a4b6dff29a283dee-115@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      imagePullPolicy: IfNotPresent
      name: proxymux-client
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/etc/proxymux/
        name: proxymux-cfg
      - mountPath: /etc/kubernetes/
        name: kubernetes-cfg
      - mountPath: /run/systemd/
        name: systemd
      - mountPath: /var/run/dbus/
        name: dbus
      - mountPath: /etc/pki/
        name: pki
      - mountPath: /etc/kubernetes/kube-root-ca/
        name: kube-root-ca
      - mountPath: /etc/kubernetes/kube-certificate-rotation/
        name: kube-certificate-rotation
      - mountPath: /var/lib/kubelet/pki
        name: var-lib-kubelet-pki
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-44zcf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: 10.0.10.209
    nodeSelector:
      node.info.ds_proxymux_client: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: proxymux-client
    serviceAccountName: proxymux-client
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/proxymux/
        type: ""
      name: proxymux-cfg
    - hostPath:
        path: /etc/kubernetes/
        type: ""
      name: kubernetes-cfg
    - hostPath:
        path: /run/systemd/
        type: ""
      name: systemd
    - hostPath:
        path: /var/run/dbus/
        type: ""
      name: dbus
    - hostPath:
        path: /etc/pki/
        type: ""
      name: pki
    - configMap:
        defaultMode: 420
        name: kube-root-ca.crt
        optional: true
      name: kube-root-ca
    - configMap:
        defaultMode: 420
        name: kube-certificate-rotation
        optional: true
      name: kube-certificate-rotation
    - hostPath:
        path: /var/lib/kubelet/pki/
        type: ""
      name: var-lib-kubelet-pki
    - name: kube-api-access-44zcf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:46Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 64Mi
      containerID: cri-o://9a694c3bfd378e1c152d3ce7d42ab39a116bd6c9e3aac3dea4f1831915477a17
      image: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      imageID: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      lastState: {}
      name: proxymux-client
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:00Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /mnt/etc/proxymux/
        name: proxymux-cfg
      - mountPath: /etc/kubernetes/
        name: kubernetes-cfg
      - mountPath: /run/systemd/
        name: systemd
      - mountPath: /var/run/dbus/
        name: dbus
      - mountPath: /etc/pki/
        name: pki
      - mountPath: /etc/kubernetes/kube-root-ca/
        name: kube-root-ca
      - mountPath: /etc/kubernetes/kube-certificate-rotation/
        name: kube-certificate-rotation
      - mountPath: /var/lib/kubelet/pki
        name: var-lib-kubelet-pki
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-44zcf
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.209
    podIPs:
    - ip: 10.0.10.209
    qosClass: Burstable
    startTime: "2025-11-14T12:57:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-14T12:57:45Z"
    generateName: proxymux-client-
    generation: 1
    labels:
      controller-revision-hash: 59d4c87876
      oke-app: proxymux-client-ds
      pod-template-generation: "1"
    name: proxymux-client-wm726
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: proxymux-client
      uid: cb2f39d0-7c13-4116-a1f3-9d3b3ae36581
    resourceVersion: "258777"
    uid: dd2b55d7-1949-4713-8236-ac692f5b2c7a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.7
    containers:
    - args:
      - --config=/mnt/etc/proxymux/config.yaml
      - --verbosity=info
      image: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli:8d4509c1e518fcb4e1a95191a4b6dff29a283dee-115@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      imagePullPolicy: IfNotPresent
      name: proxymux-client
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: true
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/etc/proxymux/
        name: proxymux-cfg
      - mountPath: /etc/kubernetes/
        name: kubernetes-cfg
      - mountPath: /run/systemd/
        name: systemd
      - mountPath: /var/run/dbus/
        name: dbus
      - mountPath: /etc/pki/
        name: pki
      - mountPath: /etc/kubernetes/kube-root-ca/
        name: kube-root-ca
      - mountPath: /etc/kubernetes/kube-certificate-rotation/
        name: kube-certificate-rotation
      - mountPath: /var/lib/kubelet/pki
        name: var-lib-kubelet-pki
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5hm57
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: 10.0.10.7
    nodeSelector:
      node.info.ds_proxymux_client: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
    serviceAccount: proxymux-client
    serviceAccountName: proxymux-client
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/proxymux/
        type: ""
      name: proxymux-cfg
    - hostPath:
        path: /etc/kubernetes/
        type: ""
      name: kubernetes-cfg
    - hostPath:
        path: /run/systemd/
        type: ""
      name: systemd
    - hostPath:
        path: /var/run/dbus/
        type: ""
      name: dbus
    - hostPath:
        path: /etc/pki/
        type: ""
      name: pki
    - configMap:
        defaultMode: 420
        name: kube-root-ca.crt
        optional: true
      name: kube-root-ca
    - configMap:
        defaultMode: 420
        name: kube-certificate-rotation
        optional: true
      name: kube-certificate-rotation
    - hostPath:
        path: /var/lib/kubelet/pki/
        type: ""
      name: var-lib-kubelet-pki
    - name: kube-api-access-5hm57
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:00Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:00Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:00Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 64Mi
      containerID: cri-o://6096cdcdcf759761fe86752a75fb53d6da3b7f15a59e68d1d2e1b5d5aa98920c
      image: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      imageID: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
      lastState: {}
      name: proxymux-client
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:58:59Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /mnt/etc/proxymux/
        name: proxymux-cfg
      - mountPath: /etc/kubernetes/
        name: kubernetes-cfg
      - mountPath: /run/systemd/
        name: systemd
      - mountPath: /var/run/dbus/
        name: dbus
      - mountPath: /etc/pki/
        name: pki
      - mountPath: /etc/kubernetes/kube-root-ca/
        name: kube-root-ca
      - mountPath: /etc/kubernetes/kube-certificate-rotation/
        name: kube-certificate-rotation
      - mountPath: /var/lib/kubelet/pki
        name: var-lib-kubelet-pki
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5hm57
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.7
    podIPs:
    - ip: 10.0.10.7
    qosClass: Burstable
    startTime: "2025-11-14T12:57:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4934a973fed4c4b4
      version_hash: "688781041"
    creationTimestamp: "2025-11-14T12:57:45Z"
    generateName: vcn-native-ip-cni-
    generation: 1
    labels:
      app: vcn-native-ip-cni
      controller-revision-hash: 68b87c5b
      pod-template-generation: "1"
      tier: node
    name: vcn-native-ip-cni-kv42m
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vcn-native-ip-cni
      uid: 82e70635-117b-4187-845e-95f98e3fcd27
    resourceVersion: "258931"
    uid: aad445d1-39dd-42dc-bf7b-b4a26c40e4e9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.209
    containers:
    - command:
      - /bin/oci-npn
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CNI_COPY_FILES
        value: "true"
      - name: PATH
        value: /hostIptables:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      - name: HAS_INIT_CONTAINER
        value: "false"
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: install-cni-ips
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /dev/shm
        name: ip-dir
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
        subPath: iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-ip6tables
        subPath: ip6tables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w78r4
        readOnly: true
    - command:
      - /bin/bash
      - -ce
      - if [ -x /bin/oci-cni-device-plugin ]; then exec /bin/oci-cni-device-plugin;
        else echo "Application resources not supported with this version of VCN Native
        IP CNI; skip device plugin creation" >&2; trap "exit 0" TERM INT; sleep infinity
        & wait; fi
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: HAS_INIT_CONTAINER
        value: "false"
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-device-plugin
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/device-plugins
        name: kubelet-device-plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w78r4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /bin/init-cni-copier
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-init-copier
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w78r4
        readOnly: true
    - command:
      - /bin/bash
      - -ce
      - echo Attempting to reach Kubernetes API server at ${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT};
        timeout 30 curl -ksSvo /dev/null --connect-timeout 5 --retry-delay 5 --retry
        999 https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}/api/v1/nodes
        || true
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-init
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w78r4
        readOnly: true
    nodeName: 10.0.10.209
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: vcn-native-ip-cni
    serviceAccountName: vcn-native-ip-cni
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - hostPath:
        path: /dev/shm
        type: ""
      name: ip-dir
    - hostPath:
        path: /var/lib/kubelet/device-plugins
        type: Directory
      name: kubelet-device-plugins
    - configMap:
        defaultMode: 420
        name: vcn-native-ip-cni-cfg
      name: cni-cfg
    - configMap:
        defaultMode: 493
        items:
        - key: iptables
          path: iptables
        name: vcn-native-ip-cni-cfg
      name: chroot-iptables
    - configMap:
        defaultMode: 493
        items:
        - key: ip6tables
          path: ip6tables
        name: vcn-native-ip-cni-cfg
      name: chroot-ip6tables
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - name: kube-api-access-w78r4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:01Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:10Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:12Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:12Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://50ef938ac216aab59e08d18d4bc44804467043c1c2bb2def6de179535e965886
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: install-cni-ips
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:11Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /dev/shm
        name: ip-dir
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-ip6tables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w78r4
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://cdd07249806ca8a7de8a4200f44d6530804370190cbe7a544675b7f29ee3e701
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-device-plugin
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:11Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kubelet/device-plugins
        name: kubelet-device-plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w78r4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    initContainerStatuses:
    - containerID: cri-o://ab0fc6064813aa8aa17504a796fd860018434dd96620d50a3183e243f3abf454
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-init-copier
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://ab0fc6064813aa8aa17504a796fd860018434dd96620d50a3183e243f3abf454
          exitCode: 0
          finishedAt: "2025-11-14T12:59:08Z"
          reason: Completed
          startedAt: "2025-11-14T12:59:00Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w78r4
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://6e737ff1c455927a14a6269c6b06e59ef86d0f551de89211796333484c201c15
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-init
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://6e737ff1c455927a14a6269c6b06e59ef86d0f551de89211796333484c201c15
          exitCode: 0
          finishedAt: "2025-11-14T12:59:10Z"
          reason: Completed
          startedAt: "2025-11-14T12:59:10Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w78r4
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.209
    podIPs:
    - ip: 10.0.10.209
    qosClass: BestEffort
    startTime: "2025-11-14T12:57:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4934a973fed4c4b4
      version_hash: "688781041"
    creationTimestamp: "2025-11-14T12:57:45Z"
    generateName: vcn-native-ip-cni-
    generation: 1
    labels:
      app: vcn-native-ip-cni
      controller-revision-hash: 68b87c5b
      pod-template-generation: "1"
      tier: node
    name: vcn-native-ip-cni-tr968
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vcn-native-ip-cni
      uid: 82e70635-117b-4187-845e-95f98e3fcd27
    resourceVersion: "258915"
    uid: 03ba8bb9-3b02-49b1-8247-64e3335dbd25
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.7
    containers:
    - command:
      - /bin/oci-npn
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CNI_COPY_FILES
        value: "true"
      - name: PATH
        value: /hostIptables:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      - name: HAS_INIT_CONTAINER
        value: "false"
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: install-cni-ips
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /dev/shm
        name: ip-dir
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
        subPath: iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-ip6tables
        subPath: ip6tables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9b4l
        readOnly: true
    - command:
      - /bin/bash
      - -ce
      - if [ -x /bin/oci-cni-device-plugin ]; then exec /bin/oci-cni-device-plugin;
        else echo "Application resources not supported with this version of VCN Native
        IP CNI; skip device plugin creation" >&2; trap "exit 0" TERM INT; sleep infinity
        & wait; fi
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: HAS_INIT_CONTAINER
        value: "false"
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-device-plugin
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/device-plugins
        name: kubelet-device-plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9b4l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /bin/init-cni-copier
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-init-copier
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9b4l
        readOnly: true
    - command:
      - /bin/bash
      - -ce
      - echo Attempting to reach Kubernetes API server at ${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT};
        timeout 30 curl -ksSvo /dev/null --connect-timeout 5 --retry-delay 5 --retry
        999 https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}/api/v1/nodes
        || true
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-init
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9b4l
        readOnly: true
    nodeName: 10.0.10.7
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: vcn-native-ip-cni
    serviceAccountName: vcn-native-ip-cni
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - hostPath:
        path: /dev/shm
        type: ""
      name: ip-dir
    - hostPath:
        path: /var/lib/kubelet/device-plugins
        type: Directory
      name: kubelet-device-plugins
    - configMap:
        defaultMode: 420
        name: vcn-native-ip-cni-cfg
      name: cni-cfg
    - configMap:
        defaultMode: 493
        items:
        - key: iptables
          path: iptables
        name: vcn-native-ip-cni-cfg
      name: chroot-iptables
    - configMap:
        defaultMode: 493
        items:
        - key: ip6tables
          path: ip6tables
        name: vcn-native-ip-cni-cfg
      name: chroot-ip6tables
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - name: kube-api-access-b9b4l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:00Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:09Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:11Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:11Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8bb9d100ef43d6b86c80f9591ed7e27606fa95aa844b2d71822316c04781409f
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: install-cni-ips
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:09Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /dev/shm
        name: ip-dir
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-ip6tables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9b4l
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://ec6d01738cdb6e7d8419652a894f391fa000b21d1bd7796c0085df29fe11596c
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-device-plugin
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:11Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kubelet/device-plugins
        name: kubelet-device-plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9b4l
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    initContainerStatuses:
    - containerID: cri-o://5e39e13c59d82599d1f6c46e12199d6ccb08aabf6f10fb47d0fbf52560486593
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-init-copier
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://5e39e13c59d82599d1f6c46e12199d6ccb08aabf6f10fb47d0fbf52560486593
          exitCode: 0
          finishedAt: "2025-11-14T12:59:07Z"
          reason: Completed
          startedAt: "2025-11-14T12:58:59Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9b4l
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://991464ee0d9a757b8196824a770d9e996b9cf0e7e619c5f8b774d4e2a41d8ed5
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-init
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://991464ee0d9a757b8196824a770d9e996b9cf0e7e619c5f8b774d4e2a41d8ed5
          exitCode: 0
          finishedAt: "2025-11-14T12:59:08Z"
          reason: Completed
          startedAt: "2025-11-14T12:59:08Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b9b4l
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.7
    podIPs:
    - ip: 10.0.10.7
    qosClass: BestEffort
    startTime: "2025-11-14T12:57:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4934a973fed4c4b4
      version_hash: "688781041"
    creationTimestamp: "2025-11-14T12:57:51Z"
    generateName: vcn-native-ip-cni-
    generation: 1
    labels:
      app: vcn-native-ip-cni
      controller-revision-hash: 68b87c5b
      pod-template-generation: "1"
      tier: node
    name: vcn-native-ip-cni-vklkn
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vcn-native-ip-cni
      uid: 82e70635-117b-4187-845e-95f98e3fcd27
    resourceVersion: "259009"
    uid: aacb74b2-3422-45fb-8ebf-a532845fda72
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.83
    containers:
    - command:
      - /bin/oci-npn
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CNI_COPY_FILES
        value: "true"
      - name: PATH
        value: /hostIptables:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
      - name: HAS_INIT_CONTAINER
        value: "false"
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: install-cni-ips
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /dev/shm
        name: ip-dir
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
        subPath: iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-ip6tables
        subPath: ip6tables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gdnx4
        readOnly: true
    - command:
      - /bin/bash
      - -ce
      - if [ -x /bin/oci-cni-device-plugin ]; then exec /bin/oci-cni-device-plugin;
        else echo "Application resources not supported with this version of VCN Native
        IP CNI; skip device plugin creation" >&2; trap "exit 0" TERM INT; sleep infinity
        & wait; fi
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: HAS_INIT_CONTAINER
        value: "false"
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-device-plugin
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/device-plugins
        name: kubelet-device-plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gdnx4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /bin/init-cni-copier
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-init-copier
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gdnx4
        readOnly: true
    - command:
      - /bin/bash
      - -ce
      - echo Attempting to reach Kubernetes API server at ${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT};
        timeout 30 curl -ksSvo /dev/null --connect-timeout 5 --retry-delay 5 --retry
        999 https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}/api/v1/nodes
        || true
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imagePullPolicy: IfNotPresent
      name: oci-cni-init
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gdnx4
        readOnly: true
    nodeName: 10.0.10.83
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: vcn-native-ip-cni
    serviceAccountName: vcn-native-ip-cni
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - hostPath:
        path: /dev/shm
        type: ""
      name: ip-dir
    - hostPath:
        path: /var/lib/kubelet/device-plugins
        type: Directory
      name: kubelet-device-plugins
    - configMap:
        defaultMode: 420
        name: vcn-native-ip-cni-cfg
      name: cni-cfg
    - configMap:
        defaultMode: 493
        items:
        - key: iptables
          path: iptables
        name: vcn-native-ip-cni-cfg
      name: chroot-iptables
    - configMap:
        defaultMode: 493
        items:
        - key: ip6tables
          path: ip6tables
        name: vcn-native-ip-cni-cfg
      name: chroot-ip6tables
    - hostPath:
        path: /
        type: Directory
      name: host-root
    - name: kube-api-access-gdnx4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:10Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:19Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:21Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:59:21Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T12:57:51Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8d9591de3ecc6ada7c455f19bdeb515cfba04b87b68336428c39107f84e646db
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: install-cni-ips
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:20Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /dev/shm
        name: ip-dir
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /host
        name: host-root
      - mountPath: /hostIptables/iptables
        name: chroot-iptables
      - mountPath: /hostIptables/ip6tables
        name: chroot-ip6tables
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gdnx4
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://34b74703089fa659a68d87e415a0d37be1c06f9802f57b8fac72619400436a08
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-device-plugin
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T12:59:21Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/kubelet/device-plugins
        name: kubelet-device-plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gdnx4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    initContainerStatuses:
    - containerID: cri-o://bb7ffade7662a98b2834d90dce2db0d17ef3dec2e0d22fd1c61c908180f7ab24
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-init-copier
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://bb7ffade7662a98b2834d90dce2db0d17ef3dec2e0d22fd1c61c908180f7ab24
          exitCode: 0
          finishedAt: "2025-11-14T12:59:18Z"
          reason: Completed
          startedAt: "2025-11-14T12:59:10Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/oci-cni
        name: cni-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gdnx4
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://fde28a70082ee8c9cf1578e41b6e5f0e42be4bac04010c8070293496df3b7fad
      image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      imageID: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
      lastState: {}
      name: oci-cni-init
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://fde28a70082ee8c9cf1578e41b6e5f0e42be4bac04010c8070293496df3b7fad
          exitCode: 0
          finishedAt: "2025-11-14T12:59:19Z"
          reason: Completed
          startedAt: "2025-11-14T12:59:19Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gdnx4
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.83
    podIPs:
    - ip: 10.0.10.83
    qosClass: BestEffort
    startTime: "2025-11-14T12:57:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2025-11-17T22:52:49Z"
    generateName: alertmanager-prometheus-alertmanager-
    generation: 1
    labels:
      alertmanager: prometheus-alertmanager
      app.kubernetes.io/instance: prometheus-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.29.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-prometheus-alertmanager-6879d8bdb5
      statefulset.kubernetes.io/pod-name: alertmanager-prometheus-alertmanager-0
    name: alertmanager-prometheus-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-prometheus-alertmanager
      uid: 271bddf4-a63a-4945-b302-bb774ed3e64d
    resourceVersion: "1599955"
    uid: 14943297-f639-435a-98f2-40d5d17c4c51
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - alertmanager
              - key: alertmanager
                operator: In
                values:
                - prometheus-alertmanager
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://prometheus-alertmanager.monitoring:9093
      - --web.route-prefix=/
      - --cluster.label=monitoring/prometheus-alertmanager
      - --cluster.peer=alertmanager-prometheus-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.29.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-prometheus-alertmanager-db
        subPath: alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
        name: cluster-tls-config
        readOnly: true
        subPath: cluster-tls-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cs5x
        readOnly: true
    - args:
      - --listen-address=:8080
      - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cs5x
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-prometheus-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-init
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cs5x
        readOnly: true
    nodeName: 10.0.10.7
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-alertmanager
    serviceAccountName: prometheus-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: alertmanager-prometheus-alertmanager-db
      persistentVolumeClaim:
        claimName: alertmanager-prometheus-alertmanager-db-alertmanager-prometheus-alertmanager-0
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-prometheus-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-prometheus-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-prometheus-alertmanager-web-config
    - name: cluster-tls-config
      secret:
        defaultMode: 420
        secretName: alertmanager-prometheus-alertmanager-cluster-tls-config
    - name: kube-api-access-5cs5x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:53:32Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:53:32Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:53:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:53:40Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:53:00Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 128Mi
      containerID: cri-o://9e767c7170cbd961bb1fc4fd44151a2590dcae74bd4698ae1398778af0a0917e
      image: quay.io/prometheus/alertmanager:v0.29.0
      imageID: quay.io/prometheus/alertmanager@sha256:86ed3780fa25d23de5110c97a63a3061e7841cef87bf5183568bc97437764af2
      lastState: {}
      name: alertmanager
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 50m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:53:37Z"
      user:
        linux:
          gid: 2000
          supplementalGroups:
          - 2000
          uid: 1000
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /alertmanager
        name: alertmanager-prometheus-alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
        name: cluster-tls-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cs5x
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://15601665c4bfe719b06b090b2aa51723951df6b68c7faa47dd7daa23e1ea685d
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:44dd821cb3dd26698c3e97b10dc22ec5707afe6126b5912dc11b169394bf2ef7
      lastState: {}
      name: config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:53:39Z"
      user:
        linux:
          gid: 2000
          supplementalGroups:
          - 2000
          uid: 1000
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cs5x
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    initContainerStatuses:
    - containerID: cri-o://29363f9bf5f7ac4fb5d2f0c842cec4c32f9d639937c04c2cd4babdc2173af2e0
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:44dd821cb3dd26698c3e97b10dc22ec5707afe6126b5912dc11b169394bf2ef7
      lastState: {}
      name: init-config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://29363f9bf5f7ac4fb5d2f0c842cec4c32f9d639937c04c2cd4babdc2173af2e0
          exitCode: 0
          finishedAt: "2025-11-17T22:53:31Z"
          reason: Completed
          startedAt: "2025-11-17T22:53:31Z"
      user:
        linux:
          gid: 2000
          supplementalGroups:
          - 2000
          uid: 1000
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cs5x
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.71
    podIPs:
    - ip: 10.0.40.71
    qosClass: Burstable
    startTime: "2025-11-17T22:53:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 90405c87629d83f9f75396f01cab18a6713e2ea1df9996b2bff192be91221389
      kubectl.kubernetes.io/default-container: loki
      storage/size: 20Gi
    creationTimestamp: "2025-11-18T00:48:51Z"
    generateName: loki-
    generation: 1
    labels:
      app.kubernetes.io/component: single-binary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: loki-7d654cb9d4
      statefulset.kubernetes.io/pod-name: loki-0
    name: loki-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki
      uid: 81bae65b-a59e-41e7-bead-f793480adf30
    resourceVersion: "1632547"
    uid: edadf441-b1d1-4cca-a771-e2be87a352ce
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: single-binary
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=all
      image: docker.io/grafana/loki:3.5.7
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 200m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /var/loki
        name: storage
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w4564
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: loki_rule
      - name: FOLDER
        value: /rules
      - name: RESOURCE
        value: both
      - name: WATCH_SERVER_TIMEOUT
        value: "60"
      - name: WATCH_CLIENT_TIMEOUT
        value: "60"
      - name: LOG_LEVEL
        value: INFO
      image: docker.io/kiwigrid/k8s-sidecar:1.30.10
      imagePullPolicy: IfNotPresent
      name: loki-sc-rules
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w4564
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-0
    nodeName: 10.0.10.209
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-loki-0
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - emptyDir: {}
      name: sc-rules-volume
    - name: kube-api-access-w4564
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:49:02Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:48:51Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:49:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:49:44Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:48:51Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 200m
        memory: 512Mi
      containerID: cri-o://4b91759a3bcf1e3d9013620ddabff76518f62d84fa5e8af4b8855c7cdc1bfa5b
      image: docker.io/grafana/loki:3.5.7
      imageID: docker.io/grafana/loki@sha256:0eaee7bf39cc83aaef46914fb58f287d4f4c4be6ec96b86c2ed55719a75e49c8
      lastState: {}
      name: loki
      ready: true
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 200m
          memory: 512Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T00:49:00Z"
      user:
        linux:
          gid: 10001
          supplementalGroups:
          - 10001
          uid: 10001
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /var/loki
        name: storage
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w4564
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://2e5aaa0c95705a3a0d87fa246cb4654e541c925f269ab376b70e82c3b96ad262
      image: docker.io/kiwigrid/k8s-sidecar:1.30.10
      imageID: docker.io/kiwigrid/k8s-sidecar@sha256:2248efa2bf19ab7b0ae6c10017c484ddbdbfe2de3c1b255ee12c2c606b9d91e1
      lastState: {}
      name: loki-sc-rules
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T00:49:01Z"
      user:
        linux:
          gid: 10001
          supplementalGroups:
          - 10001
          uid: 10001
      volumeMounts:
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w4564
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.61
    podIPs:
    - ip: 10.0.40.61
    qosClass: Burstable
    startTime: "2025-11-18T00:48:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-17T22:49:35Z"
    generateName: loki-canary-
    generation: 1
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 7f4c54dd96
      pod-template-generation: "1"
    name: loki-canary-6rvgc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: a462c30e-43de-4f00-b24e-05e097a609ea
    resourceVersion: "1598456"
    uid: 8946e30a-15c7-441b-b7cb-a1ed0f8a4a28
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.209
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      - -user=self-monitoring
      - -tenant-id=self-monitoring
      - -pass=
      - -push=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:3.5.7
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d6zbc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.209
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-d6zbc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:41Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:35Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:50:03Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:50:03Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:35Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://366dc7c8ff8369dfeb85da2004703953b7136b6439442d58a90e14b320e94d78
      image: docker.io/grafana/loki-canary:3.5.7
      imageID: docker.io/grafana/loki-canary@sha256:00e45606c63d6786d1bcf542feec2efd258ad9fdb6bbc94aa2f5e2fd966d0fff
      lastState: {}
      name: loki-canary
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:49:40Z"
      user:
        linux:
          gid: 10001
          supplementalGroups:
          - 10001
          uid: 10001
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d6zbc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.104
    podIPs:
    - ip: 10.0.40.104
    qosClass: BestEffort
    startTime: "2025-11-17T22:49:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-17T22:49:35Z"
    generateName: loki-canary-
    generation: 1
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 7f4c54dd96
      pod-template-generation: "1"
    name: loki-canary-bv4xn
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: a462c30e-43de-4f00-b24e-05e097a609ea
    resourceVersion: "1598486"
    uid: 9e9908bc-400a-40a9-bcf7-e89ca78fe78d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.83
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      - -user=self-monitoring
      - -tenant-id=self-monitoring
      - -pass=
      - -push=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:3.5.7
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mb7h7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.83
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-mb7h7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:46Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:35Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:50:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:50:07Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:35Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://651f508e52ba02eddda595c757d8c7201a3b7d552b9379e01ea857e0b3ce2f2b
      image: docker.io/grafana/loki-canary:3.5.7
      imageID: docker.io/grafana/loki-canary@sha256:00e45606c63d6786d1bcf542feec2efd258ad9fdb6bbc94aa2f5e2fd966d0fff
      lastState: {}
      name: loki-canary
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:49:45Z"
      user:
        linux:
          gid: 10001
          supplementalGroups:
          - 10001
          uid: 10001
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mb7h7
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.101
    podIPs:
    - ip: 10.0.40.101
    qosClass: BestEffort
    startTime: "2025-11-17T22:49:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-17T22:49:35Z"
    generateName: loki-canary-
    generation: 1
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 7f4c54dd96
      pod-template-generation: "1"
    name: loki-canary-qzw72
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: a462c30e-43de-4f00-b24e-05e097a609ea
    resourceVersion: "1598449"
    uid: 0921a54f-373c-4a2b-8eec-9e024a05d45c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.7
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      - -user=self-monitoring
      - -tenant-id=self-monitoring
      - -pass=
      - -push=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:3.5.7
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s2wq8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.7
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-s2wq8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:40Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:35Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:50:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:50:02Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:49:35Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8891250bdd75cb96c4187ae5e2f532d07c82a975713e010e1487bdcc4fd97bbd
      image: docker.io/grafana/loki-canary:3.5.7
      imageID: docker.io/grafana/loki-canary@sha256:00e45606c63d6786d1bcf542feec2efd258ad9fdb6bbc94aa2f5e2fd966d0fff
      lastState: {}
      name: loki-canary
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:49:39Z"
      user:
        linux:
          gid: 10001
          supplementalGroups:
          - 10001
          uid: 10001
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s2wq8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.114
    podIPs:
    - ip: 10.0.40.114
    qosClass: BestEffort
    startTime: "2025-11-17T22:49:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: ff015bd125f529abec13dae9359150ba80dd9fa80ef5698465aceb2b78b27aed
    creationTimestamp: "2025-11-18T00:30:37Z"
    generateName: loki-gateway-697968b797-
    generation: 1
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      pod-template-hash: 697968b797
    name: loki-gateway-697968b797-hl6vx
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: loki-gateway-697968b797
      uid: c3b049c8-dc12-466d-9bec-16c56913b251
    resourceVersion: "1627048"
    uid: 020047a2-8f32-4f00-8907-df2f447ccf2a
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: gateway
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    containers:
    - image: docker.io/nginxinc/nginx-unprivileged:1.29-alpine
      imagePullPolicy: IfNotPresent
      name: nginx
      ports:
      - containerPort: 8080
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx
        name: config
      - mountPath: /tmp
        name: tmp
      - mountPath: /docker-entrypoint.d
        name: docker-entrypoint-d-override
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2284f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.209
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 101
      runAsGroup: 101
      runAsNonRoot: true
      runAsUser: 101
    serviceAccount: loki
    serviceAccountName: loki
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: loki-gateway
      name: config
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: docker-entrypoint-d-override
    - name: kube-api-access-2284f
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:30:42Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:30:37Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:31:03Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:31:03Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:30:37Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 128Mi
      containerID: cri-o://8f506e8f91ff4fc23c25d251f813441044254e45e82ab1d1addc35def46f8a5a
      image: docker.io/nginxinc/nginx-unprivileged:1.29-alpine
      imageID: docker.io/nginxinc/nginx-unprivileged@sha256:24e13024ae9999412c675e7c1d4dcbafdb3c1eaa9e047ec50f6273c825db33e2
      lastState: {}
      name: nginx
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T00:30:41Z"
      user:
        linux:
          gid: 101
          supplementalGroups:
          - 101
          uid: 101
      volumeMounts:
      - mountPath: /etc/nginx
        name: config
      - mountPath: /tmp
        name: tmp
      - mountPath: /docker-entrypoint.d
        name: docker-entrypoint-d-override
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2284f
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.76
    podIPs:
    - ip: 10.0.40.76
    qosClass: Burstable
    startTime: "2025-11-18T00:30:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 5e2dee057f387b2e4a95327db5751d7cc7fa548be7adf08c4bf0d031d3da58e5
      checksum/dashboards-json-config: 9221cc5f8060f7a4e997fce086c9d7bcc5f450c0700f4044b253fa0944e9d5ee
      checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
      checksum/secret: 116bc63539f0196c60512a28c581aef2b46733356fb67f51bf2ca62b10671bf8
      kubectl.kubernetes.io/default-container: grafana
      kubectl.kubernetes.io/restartedAt: "2025-11-18T01:14:01Z"
    creationTimestamp: "2025-11-20T19:41:15Z"
    generateName: prometheus-grafana-58b9b5c88-
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
      pod-template-hash: 58b9b5c88
    name: prometheus-grafana-58b9b5c88-9r9j5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-grafana-58b9b5c88
      uid: 7c8918f3-6915-4d91-8467-dd08b1ee07c2
    resourceVersion: "2773209"
    uid: 113ce16f-70b4-43b4-8ad8-05cab5732792
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: ALL
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.30.10
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.30.10
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: prometheus-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: prometheus-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      - name: GOMEMLIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.memory
      image: docker.io/grafana/grafana:12.2.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      - containerPort: 6060
        name: profiling
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
        name: config
        subPath: dashboardproviders.yaml
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - chown
      - -R
      - 472:472
      - /var/lib/grafana
      image: docker.io/library/busybox:1.31.1
      imagePullPolicy: IfNotPresent
      name: init-chown-data
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
    - args:
      - -c
      - mkdir -p /var/lib/grafana/dashboards/default && /bin/sh -x /etc/grafana/download_dashboards.sh
      command:
      - /bin/sh
      image: docker.io/curlimages/curl:8.9.1
      imagePullPolicy: IfNotPresent
      name: download-dashboards
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/download_dashboards.sh
        name: config
        subPath: download_dashboards.sh
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
    nodeName: 10.0.10.7
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: prometheus-grafana
    serviceAccountName: prometheus-grafana
    shareProcessNamespace: false
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-grafana
      name: config
    - configMap:
        defaultMode: 420
        name: prometheus-grafana-dashboards-default
      name: dashboards-default
    - name: storage
      persistentVolumeClaim:
        claimName: prometheus-grafana
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: prometheus-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-mgr6p
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:41:19Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:41:22Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:41:41Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:41:41Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:41:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 256Mi
      containerID: cri-o://5a11eef14e41c137710c0c7c75298c36198421b2c1617450f3c2016501c21eaa
      image: docker.io/grafana/grafana:12.2.1
      imageID: docker.io/grafana/grafana@sha256:35c41e0fd0295f5d0ee5db7e780cf33506abfaf47686196f825364889dee878b
      lastState: {}
      name: grafana
      ready: true
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 256Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-20T19:41:29Z"
      user:
        linux:
          gid: 472
          supplementalGroups:
          - 472
          - 0
          uid: 472
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
        name: config
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://23575822ee176792195f10546c5f3aafd33523d208c279f4dbf02e157d39b1a3
      image: quay.io/kiwigrid/k8s-sidecar:1.30.10
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:2248efa2bf19ab7b0ae6c10017c484ddbdbfe2de3c1b255ee12c2c606b9d91e1
      lastState: {}
      name: grafana-sc-dashboard
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-20T19:41:23Z"
      user:
        linux:
          gid: 472
          supplementalGroups:
          - 472
          uid: 472
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://86978aadcb56bd38f78375dfd95854a8b1295ed30b9728789e630e07e23f85c3
      image: quay.io/kiwigrid/k8s-sidecar:1.30.10
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:2248efa2bf19ab7b0ae6c10017c484ddbdbfe2de3c1b255ee12c2c606b9d91e1
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-20T19:41:24Z"
      user:
        linux:
          gid: 472
          supplementalGroups:
          - 472
          uid: 472
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    initContainerStatuses:
    - containerID: cri-o://f8a294baa45cc54befde7eb0fa0fd2331ee1747a784193bd885518e840280663
      image: docker.io/library/busybox:1.31.1
      imageID: docker.io/library/busybox@sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209
      lastState: {}
      name: init-chown-data
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://f8a294baa45cc54befde7eb0fa0fd2331ee1747a784193bd885518e840280663
          exitCode: 0
          finishedAt: "2025-11-20T19:41:18Z"
          reason: Completed
          startedAt: "2025-11-20T19:41:18Z"
      user:
        linux:
          gid: 472
          supplementalGroups:
          - 472
          - 10
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://906f9f71bbd1232324c1a71c6ca0a1ad5c41f28f14828e9c26cb9da79f174f56
      image: docker.io/curlimages/curl:8.9.1
      imageID: docker.io/curlimages/curl@sha256:78c8580bd9480f0d2527c0b781eeb9ffa00f3795f882e625f576aa51af8f4ad5
      lastState: {}
      name: download-dashboards
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://906f9f71bbd1232324c1a71c6ca0a1ad5c41f28f14828e9c26cb9da79f174f56
          exitCode: 0
          finishedAt: "2025-11-20T19:41:21Z"
          reason: Completed
          startedAt: "2025-11-20T19:41:21Z"
      user:
        linux:
          gid: 472
          supplementalGroups:
          - 472
          uid: 472
      volumeMounts:
      - mountPath: /etc/grafana/download_dashboards.sh
        name: config
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mgr6p
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.8
    podIPs:
    - ip: 10.0.40.8
    qosClass: Burstable
    startTime: "2025-11-20T19:41:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-17T22:52:41Z"
    generateName: prometheus-kube-state-metrics-69465cd5f-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-6.4.1
      pod-template-hash: 69465cd5f
      release: prometheus
    name: prometheus-kube-state-metrics-69465cd5f-vq4lc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-kube-state-metrics-69465cd5f
      uid: 4383324c-e459-4dd5-a839-fed43a4769fe
    resourceVersion: "1599741"
    uid: 83fc4409-c3cf-40c1-83b0-d68e33cc56d6
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5tvxh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.83
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-state-metrics
    serviceAccountName: prometheus-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-5tvxh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:53Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:42Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:53:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:53:04Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 128Mi
      containerID: cri-o://ba602ea227fc2eaf8b133ab0634bd9fff7d0d53881b7bf3bff6701a7e2466cc0
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:12d340f106edf923db88bd637666c864cb9d02b53b66a3976b535750cd2ef74d
      lastState: {}
      name: kube-state-metrics
      ready: true
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:52:52Z"
      user:
        linux:
          gid: 65534
          supplementalGroups:
          - 65534
          uid: 65534
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5tvxh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.12
    podIPs:
    - ip: 10.0.40.12
    qosClass: Burstable
    startTime: "2025-11-17T22:52:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-17T22:52:41Z"
    generateName: prometheus-operator-5cf786bfc5-
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      pod-template-hash: 5cf786bfc5
      release: prometheus
    name: prometheus-operator-5cf786bfc5-fdcjw
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-operator-5cf786bfc5
      uid: 96dd10ef-5180-454f-a68d-144e5517b0ff
    resourceVersion: "1599553"
    uid: f057f4c2-c541-4313-ba84-a6a7ddcf6f98
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --kubelet-service=kube-system/prometheus-kubelet
      - --kubelet-endpoints=true
      - --kubelet-endpointslice=false
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.86.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b69zq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.83
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-operator
    serviceAccountName: prometheus-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: prometheus-admission
    - name: kube-api-access-b69zq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:47Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:42Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:49Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 128Mi
      containerID: cri-o://876d1d32f3644ce36ca1ca737412327a3e9779a16a6abfed5b50f7f56863baa3
      image: quay.io/prometheus-operator/prometheus-operator:v0.86.2
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:3fe689c2fbf78725b6a43a8ae18e71f5c939a48625bd2f61bd762edbc41ae17f
      lastState: {}
      name: kube-prometheus-stack
      ready: true
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 50m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:52:47Z"
      user:
        linux:
          gid: 65534
          supplementalGroups:
          - 65534
          uid: 65534
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b69zq
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.11
    podIPs:
    - ip: 10.0.40.11
    qosClass: Burstable
    startTime: "2025-11-17T22:52:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-11-17T22:52:41Z"
    generateName: prometheus-prometheus-node-exporter-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 6f8fb8c65c
      helm.sh/chart: prometheus-node-exporter-4.49.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: prometheus
    name: prometheus-prometheus-node-exporter-4rk5w
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: 19986cfc-c9ce-49bf-b52c-f1d628d389c6
    resourceVersion: "1599483"
    uid: d4503708-32a1-48ed-8a76-1814a5af03a2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.209
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: 10.0.10.209
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:44Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:41Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:44Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 64Mi
      containerID: cri-o://8be4dc0084ed4e29d7b124aed1c362c369aa60ea94fd1ad6b8c2f75bf29249e8
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState: {}
      name: node-exporter
      ready: true
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:52:43Z"
      user:
        linux:
          gid: 65534
          supplementalGroups:
          - 65534
          uid: 65534
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.209
    podIPs:
    - ip: 10.0.10.209
    qosClass: Burstable
    startTime: "2025-11-17T22:52:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-11-17T22:52:41Z"
    generateName: prometheus-prometheus-node-exporter-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 6f8fb8c65c
      helm.sh/chart: prometheus-node-exporter-4.49.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: prometheus
    name: prometheus-prometheus-node-exporter-l2chb
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: 19986cfc-c9ce-49bf-b52c-f1d628d389c6
    resourceVersion: "1599508"
    uid: ca895486-2fc6-4fa8-96d1-6bdf522d2a1e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.83
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: 10.0.10.83
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:45Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:41Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:45Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 64Mi
      containerID: cri-o://62fb9e73ee1b034646b15322f4540f465f519559a80e34d2de505bf21e8e19fa
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState: {}
      name: node-exporter
      ready: true
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:52:45Z"
      user:
        linux:
          gid: 65534
          supplementalGroups:
          - 65534
          uid: 65534
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.83
    podIPs:
    - ip: 10.0.10.83
    qosClass: Burstable
    startTime: "2025-11-17T22:52:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-11-17T22:52:41Z"
    generateName: prometheus-prometheus-node-exporter-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 6f8fb8c65c
      helm.sh/chart: prometheus-node-exporter-4.49.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: prometheus
    name: prometheus-prometheus-node-exporter-zdc6p
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: 19986cfc-c9ce-49bf-b52c-f1d628d389c6
    resourceVersion: "1599489"
    uid: 83d4ba2c-57ca-4662-83ca-98fc4f1c2ec7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.7
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: 10.0.10.7
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:44Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:41Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:44Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:52:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 64Mi
      containerID: cri-o://5ec56836fe72e54d829b289c43d7c4d839c89fabb4edc843a50646f885ff35e1
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState: {}
      name: node-exporter
      ready: true
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:52:44Z"
      user:
        linux:
          gid: 65534
          supplementalGroups:
          - 65534
          uid: 65534
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.10.7
    podIPs:
    - ip: 10.0.10.7
    qosClass: Burstable
    startTime: "2025-11-17T22:52:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2025-11-20T19:29:15Z"
    generateName: prometheus-prometheus-prometheus-
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 3.7.3
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-prometheus-prometheus-c569bdbd
      operator.prometheus.io/name: prometheus-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: prometheus-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-prometheus-prometheus-0
    name: prometheus-prometheus-prometheus-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-prometheus-prometheus
      uid: 92bc4520-0873-4275-b562-18b00715ee2a
    resourceVersion: "2769751"
    uid: 595eef3c-565c-494a-984e-b085fc9b0745
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - prometheus-prometheus
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.enable-remote-write-receiver
      - --web.external-url=http://prometheus-prometheus.monitoring:9090
      - --web.route-prefix=/
      - --storage.tsdb.retention.time=15d
      - --storage.tsdb.retention.size=18GB
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v3.7.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-prometheus-prometheus-db
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptr9n
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptr9n
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-prometheus-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-init
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptr9n
        readOnly: true
    nodeName: 10.0.10.83
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-prometheus
    serviceAccountName: prometheus-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: prometheus-prometheus-prometheus-db
      persistentVolumeClaim:
        claimName: prometheus-prometheus-prometheus-db-prometheus-prometheus-prometheus-0
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-prometheus-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-prometheus-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-prometheus-prometheus-rulefiles-0
      name: prometheus-prometheus-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-prometheus-prometheus-web-config
    - name: kube-api-access-ptr9n
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:29:23Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:29:23Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:29:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:29:49Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-20T19:29:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ad2532c6be5619ac90a820120a75693b476a0aee23b4eb42e0c1224094ee0130
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:44dd821cb3dd26698c3e97b10dc22ec5707afe6126b5912dc11b169394bf2ef7
      lastState: {}
      name: config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-20T19:29:24Z"
      user:
        linux:
          gid: 2000
          supplementalGroups:
          - 2000
          uid: 1000
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptr9n
        readOnly: true
        recursiveReadOnly: Disabled
    - allocatedResources:
        cpu: 500m
        memory: 1Gi
      containerID: cri-o://34d49392caddf42d2d02efb8aa05d3597fd7672e1acdc8f27da4f9a424d66677
      image: quay.io/prometheus/prometheus:v3.7.3
      imageID: quay.io/prometheus/prometheus@sha256:49214755b6153f90a597adcbff0252cc61069f8ab69ce8411285cd4a560e8038
      lastState: {}
      name: prometheus
      ready: true
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-20T19:29:23Z"
      user:
        linux:
          gid: 2000
          supplementalGroups:
          - 2000
          uid: 1000
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /prometheus
        name: prometheus-prometheus-prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptr9n
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    initContainerStatuses:
    - containerID: cri-o://5135b53208a794d5bd3a3f3d20787468dc03dd34af692f8335f95afa6a39a4b7
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:44dd821cb3dd26698c3e97b10dc22ec5707afe6126b5912dc11b169394bf2ef7
      lastState: {}
      name: init-config-reloader
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://5135b53208a794d5bd3a3f3d20787468dc03dd34af692f8335f95afa6a39a4b7
          exitCode: 0
          finishedAt: "2025-11-20T19:29:22Z"
          reason: Completed
          startedAt: "2025-11-20T19:29:22Z"
      user:
        linux:
          gid: 2000
          supplementalGroups:
          - 2000
          uid: 1000
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
        name: prometheus-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptr9n
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.78
    podIPs:
    - ip: 10.0.40.78
    qosClass: Burstable
    startTime: "2025-11-20T19:29:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 80b06baee8a3f9318aa7dfacd65ac256bb9b0b0f80a54957e8c44f4ea65cf4d1
    creationTimestamp: "2025-11-18T00:47:29Z"
    generateName: promtail-
    generation: 1
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 655bbf98b8
      pod-template-generation: "2"
    name: promtail-5kwhw
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: 0f578f10-d140-405d-bce1-60748d703571
    resourceVersion: "1631887"
    uid: 383461ac-a7a6-4265-a7f5-4e1c711987c1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.7
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4jsbb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.7
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-4jsbb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:47:31Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:47:29Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:47:42Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:47:42Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:47:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 128Mi
      containerID: cri-o://070b2194597d4206e5ad7bb88bad4d47d878098e5e56b4f7dddd50b548e45414
      image: docker.io/grafana/promtail:3.5.1
      imageID: docker.io/grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState: {}
      name: promtail
      ready: true
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T00:47:31Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4jsbb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.88
    podIPs:
    - ip: 10.0.40.88
    qosClass: Burstable
    startTime: "2025-11-18T00:47:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 80b06baee8a3f9318aa7dfacd65ac256bb9b0b0f80a54957e8c44f4ea65cf4d1
    creationTimestamp: "2025-11-18T00:46:44Z"
    generateName: promtail-
    generation: 1
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 655bbf98b8
      pod-template-generation: "2"
    name: promtail-62v46
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: 0f578f10-d140-405d-bce1-60748d703571
    resourceVersion: "1632759"
    uid: 854fdb9f-1158-4f6d-9590-2729268d1a8f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.209
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t7rb7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.209
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-t7rb7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:46:47Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:46:45Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:50:28Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:50:28Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:46:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 128Mi
      containerID: cri-o://65ad4fc4ed2c832d633b6da5ee0cc7b135d744f7110b621de570ff8786efcef5
      image: docker.io/grafana/promtail:3.5.1
      imageID: docker.io/grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState: {}
      name: promtail
      ready: true
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T00:46:47Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t7rb7
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.209
    hostIPs:
    - ip: 10.0.10.209
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.40
    podIPs:
    - ip: 10.0.40.40
    qosClass: Burstable
    startTime: "2025-11-18T00:46:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 80b06baee8a3f9318aa7dfacd65ac256bb9b0b0f80a54957e8c44f4ea65cf4d1
    creationTimestamp: "2025-11-18T00:48:14Z"
    generateName: promtail-
    generation: 1
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 655bbf98b8
      pod-template-generation: "2"
    name: promtail-frc75
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: 0f578f10-d140-405d-bce1-60748d703571
    resourceVersion: "1632137"
    uid: cc830bba-edab-4bd8-b961-a47cb9a37c7b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - 10.0.10.83
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ms9fr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: 10.0.10.83
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-ms9fr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:48:17Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:48:14Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:48:28Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:48:28Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-18T00:48:14Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 50m
        memory: 128Mi
      containerID: cri-o://a835c2e56797df1ac9814802c1f15aff0f6a43f0d709fabb99c47aa7c1b98b7f
      image: docker.io/grafana/promtail:3.5.1
      imageID: docker.io/grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState: {}
      name: promtail
      ready: true
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-18T00:48:17Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ms9fr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.83
    hostIPs:
    - ip: 10.0.10.83
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.45
    podIPs:
    - ip: 10.0.40.45
    qosClass: Burstable
    startTime: "2025-11-18T00:48:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 27e8162c5b1eab4bb959f8c48a5b0999ad352a32609ef4d0c5ff7500cd6d89d0
    creationTimestamp: "2025-11-17T22:56:16Z"
    generateName: tempo-
    generation: 1
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: tempo-845fcbc677
      statefulset.kubernetes.io/pod-name: tempo-0
    name: tempo-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: tempo
      uid: d2420b0b-014a-4ca7-bf37-8c5447ed6a42
    resourceVersion: "1601139"
    uid: a45f3e72-e7d8-4ef8-8a8b-78ee7a4c5599
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/conf/tempo.yaml
      - -mem-ballast-size-mbs=1024
      image: grafana/tempo:2.9.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 3200
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: tempo
      ports:
      - containerPort: 3200
        name: prom-metrics
        protocol: TCP
      - containerPort: 6831
        name: jaeger-thrift-c
        protocol: UDP
      - containerPort: 6832
        name: jaeger-thrift-b
        protocol: UDP
      - containerPort: 14268
        name: jaeger-thrift-h
        protocol: TCP
      - containerPort: 14250
        name: jaeger-grpc
        protocol: TCP
      - containerPort: 9411
        name: zipkin
        protocol: TCP
      - containerPort: 55680
        name: otlp-legacy
        protocol: TCP
      - containerPort: 4317
        name: otlp-grpc
        protocol: TCP
      - containerPort: 55681
        name: otlp-httplegacy
        protocol: TCP
      - containerPort: 4318
        name: otlp-http
        protocol: TCP
      - containerPort: 55678
        name: opencensus
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 3200
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /conf
        name: tempo-conf
      - mountPath: /var/tempo
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l4f2l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: tempo-0
    nodeName: 10.0.10.7
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: tempo
    serviceAccountName: tempo
    subdomain: tempo-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-tempo-0
    - configMap:
        defaultMode: 420
        name: tempo
      name: tempo-conf
    - name: kube-api-access-l4f2l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:56:58Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:56:28Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:57:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:57:40Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-17T22:56:28Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://9e0470bdf6431a0a7e65dfb0ac0f02bd3cd64ecc1e8861c93bfa9d4b5b8f4cc5
      image: docker.io/grafana/tempo:2.9.0
      imageID: docker.io/grafana/tempo@sha256:65a5789759435f1ef696f1953258b9bbdb18eb571d5ce711ff812d2e128288a4
      lastState: {}
      name: tempo
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-17T22:56:57Z"
      user:
        linux:
          gid: 10001
          supplementalGroups:
          - 10001
          - 2000
          uid: 1000
      volumeMounts:
      - mountPath: /conf
        name: tempo-conf
      - mountPath: /var/tempo
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l4f2l
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.10.7
    hostIPs:
    - ip: 10.0.10.7
    observedGeneration: 1
    phase: Running
    podIP: 10.0.40.67
    podIPs:
    - ip: 10.0.40.67
    qosClass: BestEffort
    startTime: "2025-11-17T22:56:28Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"cert-manager","app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager","app.kubernetes.io/version":"v1.13.3"},"name":"cert-manager","namespace":"cert-manager"},"spec":{"ports":[{"name":"tcp-prometheus-servicemonitor","port":9402,"protocol":"TCP","targetPort":9402}],"selector":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager"},"type":"ClusterIP"}}
    creationTimestamp: "2025-11-18T20:38:37Z"
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.3
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "1957863"
    uid: 4b371f53-db3f-4b75-bcd0-238b499f5374
  spec:
    clusterIP: 10.96.36.21
    clusterIPs:
    - 10.96.36.21
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"webhook","app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook","app.kubernetes.io/version":"v1.13.3"},"name":"cert-manager-webhook","namespace":"cert-manager"},"spec":{"ports":[{"name":"https","port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook"},"type":"ClusterIP"}}
    creationTimestamp: "2025-11-18T20:38:38Z"
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.3
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "1957867"
    uid: eae678e6-25d1-41d6-a3db-836f297c5211
  spec:
    clusterIP: 10.96.196.64
    clusterIPs:
    - 10.96.196.64
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"hotrod","namespace":"default"},"spec":{"ports":[{"port":8080,"targetPort":8080}],"selector":{"app":"hotrod"},"type":"ClusterIP"}}
    creationTimestamp: "2025-11-22T00:27:09Z"
    name: hotrod
    namespace: default
    resourceVersion: "3271523"
    uid: 59ce7f2d-e08b-4882-8eb1-08aacbc7f978
  spec:
    clusterIP: 10.96.242.4
    clusterIPs:
    - 10.96.242.4
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: hotrod
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-11-13T19:51:36Z"
    labels:
      component: apiserver
      oke-managed: "true"
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "617"
    uid: 94af8691-cb79-4a4e-94d4-15d3b1559153
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    - name: proxymux
      port: 12250
      protocol: TCP
      targetPort: 12250
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-11-18T19:57:38Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "1946173"
    uid: 1dfbef70-0a00-446e-a2cd-deffc7df4a5c
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.96.199.70
    clusterIPs:
    - 10.96.199.70
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      nodePort: 30153
      port: 80
      protocol: TCP
      targetPort: http
    - appProtocol: https
      name: https
      nodePort: 32435
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 129.80.59.143
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-11-18T19:57:38Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
    name: ingress-nginx-controller-metrics
    namespace: ingress-nginx
    resourceVersion: "1945954"
    uid: 5ca0f77e-0d7f-426e-8f8e-626a28d942d5
  spec:
    clusterIP: 10.96.210.46
    clusterIPs:
    - 10.96.210.46
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 10254
      protocol: TCP
      targetPort: metrics
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-11-18T19:57:38Z"
    labels:
      app.kubernetes.io/component: default-backend
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
    name: ingress-nginx-defaultbackend
    namespace: ingress-nginx
    resourceVersion: "1945953"
    uid: 17d898a7-b9ee-4124-b408-1a364aeb5a10
  spec:
    clusterIP: 10.96.180.132
    clusterIPs:
    - 10.96.180.132
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/component: default-backend
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-11-13T19:53:35Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "813"
    uid: 7e98634e-5b36-45c9-8b31-e7304096b645
  spec:
    clusterIP: 10.96.5.5
    clusterIPs:
    - 10.96.5.5
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9400"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-11-13T19:53:35Z"
    labels:
      k8s-app: oke-nvidia-dcgm-exporter
    name: oke-nvidia-dcgm-exporter
    namespace: kube-system
    resourceVersion: "830"
    uid: b79275d3-2a20-4c24-8137-b619882f4488
  spec:
    clusterIP: 10.96.84.112
    clusterIPs:
    - 10.96.84.112
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: oke-nvidia-dcgm-exporter
      port: 9400
      protocol: TCP
      targetPort: 9400
    selector:
      k8s-app: oke-nvidia-dcgm-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    labels:
      app: kube-prometheus-stack-coredns
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      jobLabel: coredns
      release: prometheus
    name: prometheus-coredns
    namespace: kube-system
    resourceVersion: "1599323"
    uid: 19182371-ba28-4f47-86fd-48ef391cfcb7
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-11-17T22:52:48Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: prometheus-kubelet
    namespace: kube-system
    resourceVersion: "1599535"
    uid: 5e06d657-44cb-4d30-9cc0-14c7c4c56a6b
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-11-17T22:52:49Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      managed-by: prometheus-operator
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: prometheus-alertmanager
      uid: fd82508a-d26f-406a-a528-17b855efb7c9
    resourceVersion: "1599562"
    uid: 27097247-85fc-4573-bbce-ddc92564552a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: mesh-tcp
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: mesh-udp
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-18T00:30:37Z"
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 3.5.7
      helm.sh/chart: loki-6.46.0
    name: loki
    namespace: monitoring
    resourceVersion: "1626865"
    uid: 0dd116a0-433b-487e-874c-e8b6831a7dd2
  spec:
    clusterIP: 10.96.232.177
    clusterIPs:
    - 10.96.232.177
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: grpc
    selector:
      app.kubernetes.io/component: single-binary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:49:35Z"
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 3.5.7
      helm.sh/chart: loki-6.46.0
    name: loki-canary
    namespace: monitoring
    resourceVersion: "1598215"
    uid: 307f7564-16d2-4f0e-b7f3-3a8f87be0972
  spec:
    clusterIP: 10.96.111.120
    clusterIPs:
    - 10.96.111.120
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3500
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:49:35Z"
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 3.5.7
      helm.sh/chart: loki-6.46.0
      prometheus.io/service-monitor: "false"
    name: loki-gateway
    namespace: monitoring
    resourceVersion: "1598217"
    uid: f94407c1-8dd5-40cf-942d-b38cd7dd64e5
  spec:
    clusterIP: 10.96.215.211
    clusterIPs:
    - 10.96.215.211
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 80
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-18T00:30:36Z"
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 3.5.7
      helm.sh/chart: loki-6.46.0
      prometheus.io/service-monitor: "false"
      variant: headless
    name: loki-headless
    namespace: monitoring
    resourceVersion: "1626859"
    uid: 7f09861e-93be-4802-aad9-2a9bb43a263d
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:49:35Z"
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 3.5.7
      helm.sh/chart: loki-6.46.0
    name: loki-memberlist
    namespace: monitoring
    resourceVersion: "1598212"
    uid: 8d0662be-0b9b-4817-8391-343c1440be89
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp
      port: 7946
      protocol: TCP
      targetPort: http-memberlist
    selector:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      release: prometheus
      self-monitor: "true"
    name: prometheus-alertmanager
    namespace: monitoring
    resourceVersion: "1599331"
    uid: 4a899242-9ec4-40fe-bc4c-729138eb8d2b
  spec:
    clusterIP: 10.96.19.167
    clusterIPs:
    - 10.96.19.167
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: prometheus-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
    name: prometheus-grafana
    namespace: monitoring
    resourceVersion: "1643468"
    uid: 2b60d66d-25e9-4861-9c78-c79181446ff7
  spec:
    clusterIP: 10.96.210.45
    clusterIPs:
    - 10.96.210.45
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 80
      protocol: TCP
      targetPort: grafana
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-6.4.1
      release: prometheus
    name: prometheus-kube-state-metrics
    namespace: monitoring
    resourceVersion: "1599327"
    uid: 1d39ab17-34b3-4fc4-b70f-b54c224769d6
  spec:
    clusterIP: 10.96.146.149
    clusterIPs:
    - 10.96.146.149
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-11-17T22:52:49Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: prometheus-prometheus
      uid: 4792f7c1-b96a-4c08-b06b-8805e3bb261b
    resourceVersion: "1599586"
    uid: 63c22eca-9e89-4b85-99a5-1a5f37442e66
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      release: prometheus
    name: prometheus-operator
    namespace: monitoring
    resourceVersion: "1599346"
    uid: a01c272c-3338-4a3a-9857-3343f5ac4583
  spec:
    clusterIP: 10.96.181.206
    clusterIPs:
    - 10.96.181.206
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      release: prometheus
      self-monitor: "true"
    name: prometheus-prometheus
    namespace: monitoring
    resourceVersion: "1599328"
    uid: d4d881c4-7f43-465d-836c-330c6bdadc93
  spec:
    clusterIP: 10.96.250.87
    clusterIPs:
    - 10.96.250.87
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: prometheus-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-11-17T22:52:41Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      helm.sh/chart: prometheus-node-exporter-4.49.1
      jobLabel: node-exporter
      release: prometheus
    name: prometheus-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "1599330"
    uid: 43462ecb-cd3f-4a2a-928f-437f979d17a3
  spec:
    clusterIP: 10.96.22.44
    clusterIPs:
    - 10.96.22.44
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: promtail
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:57:49Z"
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: promtail
      app.kubernetes.io/version: 3.5.1
      helm.sh/chart: promtail-6.17.1
    name: promtail-metrics
    namespace: monitoring
    resourceVersion: "1601191"
    uid: 5e8dad2e-4c3b-4295-8372-56e610ffcfbc
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3101
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:56:16Z"
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.9.0
      helm.sh/chart: tempo-1.24.0
    name: tempo
    namespace: monitoring
    resourceVersion: "1600714"
    uid: 131b0568-22b3-4086-b798-788917d017e7
  spec:
    clusterIP: 10.96.61.183
    clusterIPs:
    - 10.96.61.183
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tempo-jaeger-thrift-compact
      port: 6831
      protocol: UDP
      targetPort: 6831
    - name: tempo-jaeger-thrift-binary
      port: 6832
      protocol: UDP
      targetPort: 6832
    - name: tempo-prom-metrics
      port: 3200
      protocol: TCP
      targetPort: 3200
    - name: tempo-jaeger-thrift-http
      port: 14268
      protocol: TCP
      targetPort: 14268
    - name: grpc-tempo-jaeger
      port: 14250
      protocol: TCP
      targetPort: 14250
    - name: tempo-zipkin
      port: 9411
      protocol: TCP
      targetPort: 9411
    - name: tempo-otlp-legacy
      port: 55680
      protocol: TCP
      targetPort: 55680
    - name: tempo-otlp-http-legacy
      port: 55681
      protocol: TCP
      targetPort: 55681
    - name: grpc-tempo-otlp
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: tempo-otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: tempo-opencensus
      port: 55678
      protocol: TCP
      targetPort: 55678
    selector:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-11-13T19:53:10Z"
    generation: 1
    name: csi-oci-node
    namespace: kube-system
    resourceVersion: "258903"
    uid: 4823b240-34b4-4a27-a053-fbc54c80210a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-oci-node
    template:
      metadata:
        annotations:
          checksum/config: 6bc5cc92b481f982
        labels:
          app: csi-oci-node
          role: csi-oci
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/virtual-node
                  operator: DoesNotExist
        containers:
        - args:
          - --v=2
          - --csi-address=/csi/csi.sock
          - --kubelet-registration-path=/var/lib/kubelet/plugins/blockvolume.csi.oraclecloud.com/csi.sock
          - --endpoint=unix:///csi/csi.sock
          - --nodeid=$(KUBE_NODE_NAME)
          - --fss-endpoint=unix:///fss/csi.sock
          - --fss-csi-address=/fss/csi.sock
          - --fss-kubelet-registration-path=/var/lib/kubelet/plugins/fss.csi.oraclecloud.com/csi.sock
          - --fss-csi-driver-enabled=true
          - --lustre-endpoint=unix:///lustre/csi.sock
          - --lustre-csi-address=/lustre/csi.sock
          - --lustre-kubelet-registration-path=/var/lib/kubelet/plugins/lustre.csi.oraclecloud.com/csi.sock
          command:
          - /usr/local/bin/oci-csi-node-driver
          env:
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: PATH
            value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
          - name: LUSTRE_DRIVER_ENABLED
            value: "true"
          image: iad.ocir.io/axoxdievda5j/oke-public-cloud-provider-oci:v1.34-8f0fbf7e71e-9-csi@sha256:2eca76b52bc3198f86b839c199f67476100bc614e401dec25ac0a2310f609c28
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/sh
                - -c
                - rm -rf /registration/blockvolume.csi.oraclecloud.com /registration/blockvolume.csi.oraclecloud.com-reg.sock
                  /registration/fss.csi.oraclecloud.com /registration/fss.csi.oraclecloud.com-reg.sock
                  /registration/lustre.csi.oraclecloud.com /registration/lustre.csi.oraclecloud.com-reg.sock
          name: csi-node-driver
          resources:
            limits:
              cpu: 500m
              memory: 300Mi
            requests:
              cpu: 30m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: true
            privileged: true
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /var/lib/kubelet
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /dev
            name: device-dir
          - mountPath: /registration
            name: registration-dir
          - mountPath: /host
            mountPropagation: HostToContainer
            name: host-root
          - mountPath: /sbin/iscsiadm
            name: chroot-iscsiadm
            subPath: iscsiadm
          - mountPath: /fss
            name: fss-plugin-dir
          - mountPath: /host/var/lib/kubelet
            mountPropagation: Bidirectional
            name: encrypt-pods-mount-dir
          - mountPath: /sbin/umount.oci-fss
            name: fss-driver-mounts
            subPath: umount.oci-fss
          - mountPath: /sbin/umount
            name: fss-driver-mounts
            subPath: umount
          - mountPath: /sbin/mount
            name: fss-driver-mounts
            subPath: mount
          - mountPath: /lustre
            name: lustre-plugin-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: false
          runAsUser: 0
        serviceAccount: csi-oci-node-sa
        serviceAccountName: csi-oci-node-sa
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: DirectoryOrCreate
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/plugins/blockvolume.csi.oraclecloud.com
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /dev
            type: ""
          name: device-dir
        - hostPath:
            path: /
            type: Directory
          name: host-root
        - configMap:
            defaultMode: 493
            items:
            - key: iscsiadm
              path: iscsiadm
            name: oci-csi-iscsiadm
          name: chroot-iscsiadm
        - hostPath:
            path: /var/lib/kubelet/plugins/fss.csi.oraclecloud.com
            type: DirectoryOrCreate
          name: fss-plugin-dir
        - hostPath:
            path: /var/lib/kubelet
            type: Directory
          name: encrypt-pods-mount-dir
        - configMap:
            defaultMode: 493
            name: oci-fss-csi
          name: fss-driver-mounts
        - hostPath:
            path: /var/lib/kubelet/plugins/lustre.csi.oraclecloud.com
            type: DirectoryOrCreate
          name: lustre-plugin-dir
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-11-13T19:53:33Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "258936"
    uid: a453064c-b176-4fe6-a911-6f5a5db99786
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        annotations:
          checksum/config: 6f554ff8e6a89ae0
          version_hash: "-1572170703"
        labels:
          k8s-app: kube-proxy
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/virtual-node
                  operator: DoesNotExist
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: PATH
            value: /hostIptables:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
          image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            privileged: true
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /host
            name: host-root
          - mountPath: /hostIptables/iptables
            name: chroot-iptables
            subPath: iptables
          - mountPath: /hostIptables/iptables-save
            name: chroot-iptables
            subPath: iptables-save
          - mountPath: /hostIptables/iptables-restore
            name: chroot-iptables
            subPath: iptables-restore
          - mountPath: /hostIptables/ip6tables
            name: chroot-iptables
            subPath: ip6tables
          - mountPath: /hostIptables/ip6tables-save
            name: chroot-iptables
            subPath: ip6tables-save
          - mountPath: /hostIptables/ip6tables-restore
            name: chroot-iptables
            subPath: ip6tables-restore
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - sh
          - /var/lib/kube-proxy-config/kube_proxy_init.sh
          image: iad.ocir.io/id9y6mi8tcky/oke-public-kube-proxy@sha256:0c20d2732b207c84ff7b37df120bc484b16ec253ccf76086837f6fd6de7120dc
          imagePullPolicy: IfNotPresent
          name: init-kube-proxy
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            privileged: true
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /var/lib/kube-proxy-config
            name: kube-proxy-config-volume
          - mountPath: /var/lib/kube-proxy-k8s-version
            name: kube-proxy-k8s-version-volume
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: false
          runAsUser: 0
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - operator: Exists
        volumes:
        - emptyDir: {}
          name: kube-proxy
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy-config-volume
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /etc/oke/oke-k8s-version
            type: ""
          name: kube-proxy-k8s-version-volume
        - configMap:
            defaultMode: 493
            items:
            - key: iptables
              path: iptables
            - key: iptables-save
              path: iptables-save
            - key: iptables-restore
              path: iptables-restore
            - key: ip6tables
              path: ip6tables
            - key: ip6tables-save
              path: ip6tables-save
            - key: ip6tables-restore
              path: ip6tables-restore
            name: oci-iptables-kubeproxy
          name: chroot-iptables
        - hostPath:
            path: /
            type: Directory
          name: host-root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-11-13T19:53:34Z"
    generation: 1
    labels:
      k8s-app: node-termination-handler
    name: node-termination-handler
    namespace: kube-system
    resourceVersion: "781"
    uid: 7821d626-eebe-4bd9-940b-64179211aa4b
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: node-termination-handler
    template:
      metadata:
        annotations:
          version_hash: "-594989610"
        labels:
          k8s-app: node-termination-handler
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/virtual-node
                  operator: DoesNotExist
        containers:
        - env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: iad.ocir.io/id9y6mi8tcky/oke-public-node-termination-handler@sha256:4627130648f2b5d41ac982c792ec4511efcc45e82317f37f5a4d1a670312e4ba
          imagePullPolicy: IfNotPresent
          name: node-termination-handler
          resources:
            limits:
              cpu: 128m
              memory: 256Mi
            requests:
              cpu: 10m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          oci.oraclecloud.com/oke-is-preemptible: "true"
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: node-termination-handler
        serviceAccountName: node-termination-handler
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-11-13T19:53:34Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: nvidia-gpu-device-plugin
    name: nvidia-gpu-device-plugin
    namespace: kube-system
    resourceVersion: "790"
    uid: 5bc18943-ba90-4121-9ebe-cdad9d12704a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: nvidia-gpu-device-plugin
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          version_hash: "1574023390"
        labels:
          k8s-app: nvidia-gpu-device-plugin
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nvidia.com/gpu
                  operator: In
                  values:
                  - "true"
                - key: oci.oraclecloud.com/disable-gpu-device-plugin
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/virtual-node
                  operator: DoesNotExist
              - matchExpressions:
                - key: beta.kubernetes.io/instance-type
                  operator: In
                  values:
                  - BM.GPU2.2
                  - BM.GPU3.8
                  - BM.GPU4.8
                  - VM.GPU2.1
                  - VM.GPU3.1
                  - VM.GPU3.2
                  - VM.GPU3.4
                  - BM.GPU.T1.2
                  - BM.GPU.T1-2.4
                  - BM.GPU.A100-v2.8
                  - BM.GPU.GM4.8
                  - BM.GPU.A10.4
                  - BM.GPU.GU1.4
                  - VM.GPU.A10.1
                  - VM.GPU.GU1.1
                  - VM.GPU.A10.2
                  - VM.GPU.GU1.2
                  - BM.GPU.B4.8
                  - BM.GPU.H100.8
                  - BM.GPU.L40S.4
                  - VM.GPU.L40S.1
                  - VM.GPU.L40S.2
                  - VM.GPU.A100.40G.1
                  - VM.GPU.A100.40G.2
                  - VM.GPU.A100.40G.4
                  - VM.GPU.A100.40G.8
                  - VM.GPU.A100.B40G.1
                  - VM.GPU.A100.B40G.2
                  - VM.GPU.A100.B40G.4
                  - VM.GPU.A100.80G.1
                  - VM.GPU.A100.80G.2
                  - VM.GPU.A100.80G.4
                  - VM.GPU.A100.80G.8
                  - BM.GPU.L40S-NC.4
                  - BM.GPU.H100T.8
                - key: oci.oraclecloud.com/disable-gpu-device-plugin
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/virtual-node
                  operator: DoesNotExist
        containers:
        - command:
          - nvidia-device-plugin
          - --pass-device-specs=true
          image: iad.ocir.io/id9y6mi8tcky/oke-public-k8s-device-plugin@sha256:0e3c25b170348a55cac61f9768793b51e1a89b153e5ae00e7186911246ada326
          imagePullPolicy: IfNotPresent
          name: nvidia-gpu-device-plugin
          resources:
            limits:
              cpu: 50m
              memory: 200Mi
            requests:
              cpu: 50m
              memory: 200Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kubelet/device-plugins
            name: device-plugin
          - mountPath: /dev
            name: dev
          - mountPath: /config
            name: nvidia-plugin-config-volume
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: oke-nvidia-device-plugin
        serviceAccountName: oke-nvidia-device-plugin
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/kubelet/device-plugins
            type: ""
          name: device-plugin
        - hostPath:
            path: /dev
            type: ""
          name: dev
        - configMap:
            defaultMode: 420
            name: nvidia-device-plugin-config
            optional: true
          name: nvidia-plugin-config-volume
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-11-13T19:53:35Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: oke-nvidia-dcgm-exporter
    name: oke-nvidia-dcgm-exporter
    namespace: kube-system
    resourceVersion: "827"
    uid: 8120c8e8-a0c1-4d41-b142-f6f4045b33e1
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: oke-nvidia-dcgm-exporter
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          version_hash: "1574023390"
        labels:
          k8s-app: oke-nvidia-dcgm-exporter
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nvidia.com/gpu
                  operator: In
                  values:
                  - "true"
                - key: oci.oraclecloud.com/disable-gpu-device-plugin
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/virtual-node
                  operator: DoesNotExist
              - matchExpressions:
                - key: beta.kubernetes.io/instance-type
                  operator: In
                  values:
                  - BM.GPU2.2
                  - BM.GPU3.8
                  - BM.GPU4.8
                  - VM.GPU2.1
                  - VM.GPU3.1
                  - VM.GPU3.2
                  - VM.GPU3.4
                  - BM.GPU.T1.2
                  - BM.GPU.T1-2.4
                  - BM.GPU.A100-v2.8
                  - BM.GPU.GM4.8
                  - BM.GPU.A10.4
                  - BM.GPU.GU1.4
                  - VM.GPU.A10.1
                  - VM.GPU.GU1.1
                  - VM.GPU.A10.2
                  - VM.GPU.GU1.2
                  - BM.GPU.B4.8
                  - BM.GPU.H100.8
                  - BM.GPU.L40S.4
                  - VM.GPU.L40S.1
                  - VM.GPU.L40S.2
                  - VM.GPU.A100.40G.1
                  - VM.GPU.A100.40G.2
                  - VM.GPU.A100.40G.4
                  - VM.GPU.A100.40G.8
                  - VM.GPU.A100.B40G.1
                  - VM.GPU.A100.B40G.2
                  - VM.GPU.A100.B40G.4
                  - VM.GPU.A100.80G.1
                  - VM.GPU.A100.80G.2
                  - VM.GPU.A100.80G.4
                  - VM.GPU.A100.80G.8
                  - BM.GPU.L40S-NC.4
                  - BM.GPU.H100T.8
                - key: oci.oraclecloud.com/disable-gpu-device-plugin
                  operator: NotIn
                  values:
                  - "true"
                - key: node-role.kubernetes.io/virtual-node
                  operator: DoesNotExist
        containers:
        - command:
          - dcgm-exporter
          env:
          - name: NVIDIA_VISIBLE_DEVICES
            value: all
          - name: NVIDIA_DRIVER_CAPABILITIES
            value: utility
          - name: DCGM_EXPORTER_LISTEN
            value: :9400
          - name: DCGM_EXPORTER_KUBERNETES
            value: "true"
          image: iad.ocir.io/id9y6mi8tcky/oke-public-nvidia-dcgm-exporter@sha256:b1ccc3b0cc3cd5575e0f65ef46084d6a999d696ffe92eb6c7a09e6a485f423d1
          imagePullPolicy: IfNotPresent
          name: oke-nvidia-dcgm-exporter
          ports:
          - containerPort: 9400
            name: metrics
            protocol: TCP
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kubelet/device-plugins
            name: device-plugin
          - mountPath: /dev
            name: dev
          - mountPath: /var/lib/kubelet/pod-resources
            name: kubelet-pod-resources
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: oke-nvidia-dcgm-exporter
        serviceAccountName: oke-nvidia-dcgm-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/kubelet/device-plugins
            type: ""
          name: device-plugin
        - hostPath:
            path: /dev
            type: ""
          name: dev
        - hostPath:
            path: /var/lib/kubelet/pod-resources
            type: Directory
          name: kubelet-pod-resources
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2025-11-13T19:53:10Z"
    generation: 1
    labels:
      oke-app: proxymux-client-ds
    name: proxymux-client
    namespace: kube-system
    resourceVersion: "258922"
    uid: cb2f39d0-7c13-4116-a1f3-9d3b3ae36581
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        oke-app: proxymux-client-ds
    template:
      metadata:
        labels:
          oke-app: proxymux-client-ds
      spec:
        containers:
        - args:
          - --config=/mnt/etc/proxymux/config.yaml
          - --verbosity=info
          image: iad.ocir.io/axoxdievda5j/oke-public-proxymux-cli:8d4509c1e518fcb4e1a95191a4b6dff29a283dee-115@sha256:866d637f98a0e5451816d6992f90571e95d638d49636dbb7d9eaee873fb122f0
          imagePullPolicy: IfNotPresent
          name: proxymux-client
          resources:
            limits:
              cpu: 500m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: true
            privileged: true
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mnt/etc/proxymux/
            name: proxymux-cfg
          - mountPath: /etc/kubernetes/
            name: kubernetes-cfg
          - mountPath: /run/systemd/
            name: systemd
          - mountPath: /var/run/dbus/
            name: dbus
          - mountPath: /etc/pki/
            name: pki
          - mountPath: /etc/kubernetes/kube-root-ca/
            name: kube-root-ca
          - mountPath: /etc/kubernetes/kube-certificate-rotation/
            name: kube-certificate-rotation
          - mountPath: /var/lib/kubelet/pki
            name: var-lib-kubelet-pki
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          node.info.ds_proxymux_client: "true"
        priority: 2000001000
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: false
          runAsUser: 0
        serviceAccount: proxymux-client
        serviceAccountName: proxymux-client
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - operator: Exists
        volumes:
        - hostPath:
            path: /etc/proxymux/
            type: ""
          name: proxymux-cfg
        - hostPath:
            path: /etc/kubernetes/
            type: ""
          name: kubernetes-cfg
        - hostPath:
            path: /run/systemd/
            type: ""
          name: systemd
        - hostPath:
            path: /var/run/dbus/
            type: ""
          name: dbus
        - hostPath:
            path: /etc/pki/
            type: ""
          name: pki
        - configMap:
            defaultMode: 420
            name: kube-root-ca.crt
            optional: true
          name: kube-root-ca
        - configMap:
            defaultMode: 420
            name: kube-certificate-rotation
            optional: true
          name: kube-certificate-rotation
        - hostPath:
            path: /var/lib/kubelet/pki/
            type: ""
          name: var-lib-kubelet-pki
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-11-13T19:53:34Z"
    generation: 1
    labels:
      app: vcn-native-ip-cni
      tier: node
    name: vcn-native-ip-cni
    namespace: kube-system
    resourceVersion: "259010"
    uid: 82e70635-117b-4187-845e-95f98e3fcd27
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: vcn-native-ip-cni
    template:
      metadata:
        annotations:
          checksum/config: 4934a973fed4c4b4
          version_hash: "688781041"
        labels:
          app: vcn-native-ip-cni
          tier: node
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: oci.oraclecloud.com/vcn-native-ip-cni
                  operator: In
                  values:
                  - "true"
                - key: node-role.kubernetes.io/virtual-node
                  operator: DoesNotExist
        containers:
        - command:
          - /bin/oci-npn
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CNI_COPY_FILES
            value: "true"
          - name: PATH
            value: /hostIptables:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/host/usr/bin:/host/sbin
          - name: HAS_INIT_CONTAINER
            value: "false"
          image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
          imagePullPolicy: IfNotPresent
          name: install-cni-ips
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/cni/bin
            name: cni-plugin
          - mountPath: /dev/shm
            name: ip-dir
          - mountPath: /etc/cni/net.d
            name: cni
          - mountPath: /etc/oci-cni
            name: cni-cfg
          - mountPath: /host
            name: host-root
          - mountPath: /hostIptables/iptables
            name: chroot-iptables
            subPath: iptables
          - mountPath: /hostIptables/ip6tables
            name: chroot-ip6tables
            subPath: ip6tables
        - command:
          - /bin/bash
          - -ce
          - if [ -x /bin/oci-cni-device-plugin ]; then exec /bin/oci-cni-device-plugin;
            else echo "Application resources not supported with this version of VCN
            Native IP CNI; skip device plugin creation" >&2; trap "exit 0" TERM INT;
            sleep infinity & wait; fi
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: HAS_INIT_CONTAINER
            value: "false"
          image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
          imagePullPolicy: IfNotPresent
          name: oci-cni-device-plugin
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kubelet/device-plugins
            name: kubelet-device-plugins
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - /bin/init-cni-copier
          image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
          imagePullPolicy: IfNotPresent
          name: oci-cni-init-copier
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/cni/bin
            name: cni-plugin
          - mountPath: /etc/cni/net.d
            name: cni
          - mountPath: /etc/oci-cni
            name: cni-cfg
        - command:
          - /bin/bash
          - -ce
          - echo Attempting to reach Kubernetes API server at ${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT};
            timeout 30 curl -ksSvo /dev/null --connect-timeout 5 --retry-delay 5 --retry
            999 https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}/api/v1/nodes
            || true
          image: iad.ocir.io/id9y6mi8tcky/oke-public-vcn-native-ip-cni-plugin:3.0.0-1@sha256:1d5b91da16316c62e1d275b69d780417129ced5d0f8c652deff5820bf1d3ae50
          imagePullPolicy: IfNotPresent
          name: oci-cni-init
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: vcn-native-ip-cni
        serviceAccountName: vcn-native-ip-cni
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /opt/cni/bin
            type: ""
          name: cni-plugin
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni
        - hostPath:
            path: /dev/shm
            type: ""
          name: ip-dir
        - hostPath:
            path: /var/lib/kubelet/device-plugins
            type: Directory
          name: kubelet-device-plugins
        - configMap:
            defaultMode: 420
            name: vcn-native-ip-cni-cfg
          name: cni-cfg
        - configMap:
            defaultMode: 493
            items:
            - key: iptables
              path: iptables
            name: vcn-native-ip-cni-cfg
          name: chroot-iptables
        - configMap:
            defaultMode: 493
            items:
            - key: ip6tables
              path: ip6tables
            name: vcn-native-ip-cni-cfg
          name: chroot-ip6tables
        - hostPath:
            path: /
            type: Directory
          name: host-root
    updateStrategy:
      type: OnDelete
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:49:35Z"
    generation: 1
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 3.5.7
      helm.sh/chart: loki-6.46.0
    name: loki-canary
    namespace: monitoring
    resourceVersion: "1598489"
    uid: a462c30e-43de-4f00-b24e-05e097a609ea
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: canary
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    template:
      metadata:
        labels:
          app.kubernetes.io/component: canary
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
      spec:
        containers:
        - args:
          - -addr=loki-gateway.monitoring.svc.cluster.local.:80
          - -labelname=pod
          - -labelvalue=$(POD_NAME)
          - -user=self-monitoring
          - -tenant-id=self-monitoring
          - -pass=
          - -push=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: docker.io/grafana/loki-canary:3.5.7
          imagePullPolicy: IfNotPresent
          name: loki-canary
          ports:
          - containerPort: 3500
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki-canary
        serviceAccountName: loki-canary
        terminationGracePeriodSeconds: 30
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      helm.sh/chart: prometheus-node-exporter-4.49.1
      release: prometheus
    name: prometheus-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "1599512"
    uid: 19986cfc-c9ce-49bf-b52c-f1d628d389c6
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.10.2
          helm.sh/chart: prometheus-node-exporter-4.49.1
          jobLabel: node-exporter
          release: prometheus
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-node-exporter
        serviceAccountName: prometheus-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
      meta.helm.sh/release-name: promtail
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:57:49Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: promtail
      app.kubernetes.io/version: 3.5.1
      helm.sh/chart: promtail-6.17.1
    name: promtail
    namespace: monitoring
    resourceVersion: "1632761"
    uid: 0f578f10-d140-405d-bce1-60748d703571
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: promtail
        app.kubernetes.io/name: promtail
    template:
      metadata:
        annotations:
          checksum/config: 80b06baee8a3f9318aa7dfacd65ac256bb9b0b0f80a54957e8c44f4ea65cf4d1
        labels:
          app.kubernetes.io/instance: promtail
          app.kubernetes.io/name: promtail
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/etc/promtail/promtail.yaml
          env:
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/grafana/promtail:3.5.1
          imagePullPolicy: IfNotPresent
          name: promtail
          ports:
          - containerPort: 3101
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/promtail
            name: config
          - mountPath: /run/promtail
            name: run
          - mountPath: /var/lib/docker/containers
            name: containers
            readOnly: true
          - mountPath: /var/log/pods
            name: pods
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 0
          runAsUser: 0
        serviceAccount: promtail
        serviceAccountName: promtail
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: promtail
        - hostPath:
            path: /run/promtail
            type: ""
          name: run
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: containers
        - hostPath:
            path: /var/log/pods
            type: ""
          name: pods
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 2
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"cert-manager","app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager","app.kubernetes.io/version":"v1.13.3"},"name":"cert-manager","namespace":"cert-manager"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager"}},"template":{"metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"9402","prometheus.io/scrape":"true"},"labels":{"app":"cert-manager","app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager","app.kubernetes.io/version":"v1.13.3"}},"spec":{"containers":[{"args":["--v=2","--cluster-resource-namespace=$(POD_NAMESPACE)","--leader-election-namespace=kube-system","--acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.3","--max-concurrent-challenges=60"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"quay.io/jetstack/cert-manager-controller:v1.13.3","imagePullPolicy":"IfNotPresent","name":"cert-manager-controller","ports":[{"containerPort":9402,"name":"http-metrics","protocol":"TCP"},{"containerPort":9403,"name":"http-healthz","protocol":"TCP"}],"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]}}}],"enableServiceLinks":false,"nodeSelector":{"kubernetes.io/os":"linux"},"securityContext":{"runAsNonRoot":true,"seccompProfile":{"type":"RuntimeDefault"}},"serviceAccountName":"cert-manager"}}}}
    creationTimestamp: "2025-11-18T20:38:38Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.3
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "1957984"
    uid: b96283d4-3c21-4c19-be7c-be709278669f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.13.3
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.3
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.13.3
          imagePullPolicy: IfNotPresent
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-18T20:38:47Z"
      lastUpdateTime: "2025-11-18T20:38:47Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-18T20:38:38Z"
      lastUpdateTime: "2025-11-18T20:38:47Z"
      message: ReplicaSet "cert-manager-7d678bfb4f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"cainjector","app.kubernetes.io/component":"cainjector","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cainjector","app.kubernetes.io/version":"v1.13.3"},"name":"cert-manager-cainjector","namespace":"cert-manager"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/component":"cainjector","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cainjector"}},"template":{"metadata":{"labels":{"app":"cainjector","app.kubernetes.io/component":"cainjector","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cainjector","app.kubernetes.io/version":"v1.13.3"}},"spec":{"containers":[{"args":["--v=2","--leader-election-namespace=kube-system"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"quay.io/jetstack/cert-manager-cainjector:v1.13.3","imagePullPolicy":"IfNotPresent","name":"cert-manager-cainjector","securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]}}}],"enableServiceLinks":false,"nodeSelector":{"kubernetes.io/os":"linux"},"securityContext":{"runAsNonRoot":true,"seccompProfile":{"type":"RuntimeDefault"}},"serviceAccountName":"cert-manager-cainjector"}}}}
    creationTimestamp: "2025-11-18T20:38:38Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.13.3
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "1957958"
    uid: 804308e7-256c-4b89-a032-526e8f36c04e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.13.3
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.13.3
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-18T20:38:45Z"
      lastUpdateTime: "2025-11-18T20:38:45Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-18T20:38:38Z"
      lastUpdateTime: "2025-11-18T20:38:45Z"
      message: ReplicaSet "cert-manager-cainjector-7449dc67b9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"webhook","app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook","app.kubernetes.io/version":"v1.13.3"},"name":"cert-manager-webhook","namespace":"cert-manager"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook"}},"template":{"metadata":{"labels":{"app":"webhook","app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook","app.kubernetes.io/version":"v1.13.3"}},"spec":{"containers":[{"args":["--v=2","--secure-port=10250","--dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)","--dynamic-serving-ca-secret-name=cert-manager-webhook-ca","--dynamic-serving-dns-names=cert-manager-webhook","--dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)","--dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"quay.io/jetstack/cert-manager-webhook:v1.13.3","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/livez","port":6080,"scheme":"HTTP"},"initialDelaySeconds":60,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"name":"cert-manager-webhook","ports":[{"containerPort":10250,"name":"https","protocol":"TCP"},{"containerPort":6080,"name":"healthcheck","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/healthz","port":6080,"scheme":"HTTP"},"initialDelaySeconds":5,"periodSeconds":5,"successThreshold":1,"timeoutSeconds":1},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]}}}],"enableServiceLinks":false,"nodeSelector":{"kubernetes.io/os":"linux"},"securityContext":{"runAsNonRoot":true,"seccompProfile":{"type":"RuntimeDefault"}},"serviceAccountName":"cert-manager-webhook"}}}}
    creationTimestamp: "2025-11-18T20:38:39Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.3
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "1958017"
    uid: 9f407024-e039-42c8-9d27-e34b2f414bb0
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.13.3
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.13.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-18T20:38:52Z"
      lastUpdateTime: "2025-11-18T20:38:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-18T20:38:39Z"
      lastUpdateTime: "2025-11-18T20:38:52Z"
      message: ReplicaSet "cert-manager-webhook-7789f864b7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-11-18T19:57:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "1952169"
    uid: ab79e416-a47b-4e6b-9cff-a39b6686d8ab
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.14.0
          helm.sh/chart: ingress-nginx-4.14.0
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=$(POD_NAMESPACE)/ingress-nginx-defaultbackend
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --enable-metrics=true
          - --enable-ssl-passthrough
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.14.0@sha256:e4127065d0317bd11dc64c4dd38dcf7fb1c3d72e468110b4086e636dbaac943d
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 82
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-18T19:58:03Z"
      lastUpdateTime: "2025-11-18T19:58:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-18T19:57:38Z"
      lastUpdateTime: "2025-11-18T20:18:40Z"
      message: ReplicaSet "ingress-nginx-controller-764b5b4897" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-11-18T19:57:38Z"
    generation: 1
    labels:
      app.kubernetes.io/component: default-backend
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
    name: ingress-nginx-defaultbackend
    namespace: ingress-nginx
    resourceVersion: "1946047"
    uid: 51edbccc-d83a-4a41-8c79-e4065f758077
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: default-backend
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app.kubernetes.io/component: default-backend
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.14.0
          helm.sh/chart: ingress-nginx-4.14.0
      spec:
        automountServiceAccountToken: true
        containers:
        - image: registry.k8s.io/defaultbackend-amd64:1.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: ingress-nginx-default-backend
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 50m
              memory: 50Mi
            requests:
              cpu: 10m
              memory: 20Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx-backend
        serviceAccountName: ingress-nginx-backend
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-18T19:57:45Z"
      lastUpdateTime: "2025-11-18T19:57:45Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-18T19:57:38Z"
      lastUpdateTime: "2025-11-18T19:57:45Z"
      message: ReplicaSet "ingress-nginx-defaultbackend-6b98b5cfbb" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-13T19:53:34Z"
    generation: 2
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
    name: coredns
    namespace: kube-system
    resourceVersion: "259209"
    uid: c699e372-1dd5-4ae3-97d7-e93f0eed443c
  spec:
    progressDeadlineSeconds: 600
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          version_hash: "1960218013"
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:e32e8482ef16dbfd86896ece95e81111d5cb110811a65c3ce85df0ce2b69ca17
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - key: oci.oraclecloud.com/oke-is-preemptible
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 3
    conditions:
    - lastTransitionTime: "2025-11-14T12:59:20Z"
      lastUpdateTime: "2025-11-14T12:59:20Z"
      message: ReplicaSet "coredns-98c69cbf4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-11-14T12:59:44Z"
      lastUpdateTime: "2025-11-14T12:59:44Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 3
    replicas: 3
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-13T19:53:36Z"
    generation: 1
    labels:
      k8s-app: kube-dns-autoscaler
    name: kube-dns-autoscaler
    namespace: kube-system
    resourceVersion: "259104"
    uid: 9660865c-239d-45a7-bdf2-7b003e96df17
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns-autoscaler
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          version_hash: "1960218013"
        labels:
          k8s-app: kube-dns-autoscaler
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=kube-dns-autoscaler
          - --target=deployment/coredns
          - --logtostderr=true
          - --v=2
          image: iad.ocir.io/id9y6mi8tcky/oke-public-cluster-proportional-autoscaler-amd64@sha256:1908914e0c9055edd754a633de2a37fd6811a64565317f2f44bf4adea85f0654
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: dns-autoscaler
        serviceAccountName: dns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: oci.oraclecloud.com/oke-is-preemptible
          operator: Exists
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-14T12:59:35Z"
      lastUpdateTime: "2025-11-14T12:59:35Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-14T12:59:35Z"
      lastUpdateTime: "2025-11-14T12:59:35Z"
      message: ReplicaSet "kube-dns-autoscaler-6d5986cc55" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:49:35Z"
    generation: 2
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 3.5.7
      helm.sh/chart: loki-6.46.0
    name: loki-gateway
    namespace: monitoring
    resourceVersion: "1627065"
    uid: d1a3698c-7e52-46a7-a0bc-d8ee653a30ae
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: gateway
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: ff015bd125f529abec13dae9359150ba80dd9fa80ef5698465aceb2b78b27aed
        labels:
          app.kubernetes.io/component: gateway
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: gateway
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        containers:
        - image: docker.io/nginxinc/nginx-unprivileged:1.29-alpine
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 8080
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: config
          - mountPath: /tmp
            name: tmp
          - mountPath: /docker-entrypoint.d
            name: docker-entrypoint-d-override
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 101
          runAsGroup: 101
          runAsNonRoot: true
          runAsUser: 101
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: loki-gateway
          name: config
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: docker-entrypoint-d-override
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-17T22:50:03Z"
      lastUpdateTime: "2025-11-17T22:50:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-17T22:49:35Z"
      lastUpdateTime: "2025-11-18T00:31:03Z"
      message: ReplicaSet "loki-gateway-697968b797" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "7"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    generation: 7
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
    name: prometheus-grafana
    namespace: monitoring
    resourceVersion: "2773222"
    uid: b7c75b5d-7e74-4ae2-ab70-dadf3ec0410f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 5e2dee057f387b2e4a95327db5751d7cc7fa548be7adf08c4bf0d031d3da58e5
          checksum/dashboards-json-config: 9221cc5f8060f7a4e997fce086c9d7bcc5f450c0700f4044b253fa0944e9d5ee
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 116bc63539f0196c60512a28c581aef2b46733356fb67f51bf2ca62b10671bf8
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-11-18T01:14:01Z"
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.2.1
          helm.sh/chart: grafana-10.1.4
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          image: docker.io/grafana/grafana:12.2.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
            name: config
            subPath: dashboardproviders.yaml
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        - args:
          - -c
          - mkdir -p /var/lib/grafana/dashboards/default && /bin/sh -x /etc/grafana/download_dashboards.sh
          command:
          - /bin/sh
          image: docker.io/curlimages/curl:8.9.1
          imagePullPolicy: IfNotPresent
          name: download-dashboards
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/download_dashboards.sh
            name: config
            subPath: download_dashboards.sh
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-dashboards-default
          name: dashboards-default
        - name: storage
          persistentVolumeClaim:
            claimName: prometheus-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-19T01:51:48Z"
      lastUpdateTime: "2025-11-19T01:51:48Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-20T19:41:15Z"
      lastUpdateTime: "2025-11-20T19:41:41Z"
      message: ReplicaSet "prometheus-grafana-58b9b5c88" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-6.4.1
      release: prometheus
    name: prometheus-kube-state-metrics
    namespace: monitoring
    resourceVersion: "1599745"
    uid: ab44d8c3-d780-4d66-ba94-c2ccd61ba15c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.17.0
          helm.sh/chart: kube-state-metrics-6.4.1
          release: prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-17T22:53:04Z"
      lastUpdateTime: "2025-11-17T22:53:04Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-17T22:52:41Z"
      lastUpdateTime: "2025-11-17T22:53:04Z"
      message: ReplicaSet "prometheus-kube-state-metrics-69465cd5f" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      release: prometheus
    name: prometheus-operator
    namespace: monitoring
    resourceVersion: "1599560"
    uid: 71bf0d38-3bcb-4b57-b167-a0ecdde45642
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: prometheus
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 79.5.0
          chart: kube-prometheus-stack-79.5.0
          heritage: Helm
          release: prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/prometheus-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.86.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-operator
        serviceAccountName: prometheus-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: prometheus-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-17T22:52:49Z"
      lastUpdateTime: "2025-11-17T22:52:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-17T22:52:41Z"
      lastUpdateTime: "2025-11-17T22:52:49Z"
      message: ReplicaSet "prometheus-operator-5cf786bfc5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-18T20:38:38Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.13.3
      pod-template-hash: 7d678bfb4f
    name: cert-manager-7d678bfb4f
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: b96283d4-3c21-4c19-be7c-be709278669f
    resourceVersion: "1957983"
    uid: c50ea17a-07ca-4f38-9046-93561098b713
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 7d678bfb4f
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.13.3
          pod-template-hash: 7d678bfb4f
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.13.3
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.13.3
          imagePullPolicy: IfNotPresent
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-18T20:38:38Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.13.3
      pod-template-hash: 7449dc67b9
    name: cert-manager-cainjector-7449dc67b9
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 804308e7-256c-4b89-a032-526e8f36c04e
    resourceVersion: "1957957"
    uid: c511d8cc-c455-45aa-b2b3-97bdd5024bbb
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 7449dc67b9
    template:
      metadata:
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.13.3
          pod-template-hash: 7449dc67b9
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.13.3
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-18T20:38:39Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.13.3
      pod-template-hash: 7789f864b7
    name: cert-manager-webhook-7789f864b7
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: 9f407024-e039-42c8-9d27-e34b2f414bb0
    resourceVersion: "1958015"
    uid: 5b849ad6-ad97-4cb9-9671-d41ddba768fa
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 7789f864b7
    template:
      metadata:
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.13.3
          pod-template-hash: 7789f864b7
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.13.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-11-18T19:57:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
      pod-template-hash: 55c7c56df9
    name: ingress-nginx-controller-55c7c56df9
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ingress-nginx-controller
      uid: ab79e416-a47b-4e6b-9cff-a39b6686d8ab
    resourceVersion: "1952168"
    uid: 1b8d7fce-a6ac-444a-90db-6f33089d628c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        pod-template-hash: 55c7c56df9
    template:
      metadata:
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.14.0
          helm.sh/chart: ingress-nginx-4.14.0
          pod-template-hash: 55c7c56df9
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=$(POD_NAMESPACE)/ingress-nginx-defaultbackend
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          - --enable-metrics=true
          - --enable-ssl-passthrough
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.14.0@sha256:e4127065d0317bd11dc64c4dd38dcf7fb1c3d72e468110b4086e636dbaac943d
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 82
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-11-18T20:18:16Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
      pod-template-hash: 764b5b4897
    name: ingress-nginx-controller-764b5b4897
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ingress-nginx-controller
      uid: ab79e416-a47b-4e6b-9cff-a39b6686d8ab
    resourceVersion: "1952150"
    uid: 33b8cd56-ad1c-438f-a5cb-fb6f30302a20
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        pod-template-hash: 764b5b4897
    template:
      metadata:
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.14.0
          helm.sh/chart: ingress-nginx-4.14.0
          pod-template-hash: 764b5b4897
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=$(POD_NAMESPACE)/ingress-nginx-defaultbackend
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --enable-metrics=true
          - --enable-ssl-passthrough
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.14.0@sha256:e4127065d0317bd11dc64c4dd38dcf7fb1c3d72e468110b4086e636dbaac943d
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 82
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-11-18T19:57:38Z"
    generation: 1
    labels:
      app.kubernetes.io/component: default-backend
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.0
      helm.sh/chart: ingress-nginx-4.14.0
      pod-template-hash: 6b98b5cfbb
    name: ingress-nginx-defaultbackend-6b98b5cfbb
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: ingress-nginx-defaultbackend
      uid: 51edbccc-d83a-4a41-8c79-e4065f758077
    resourceVersion: "1946046"
    uid: a088a9ef-aa0f-4c4b-88a4-75985269f0b5
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: default-backend
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        pod-template-hash: 6b98b5cfbb
    template:
      metadata:
        labels:
          app.kubernetes.io/component: default-backend
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.14.0
          helm.sh/chart: ingress-nginx-4.14.0
          pod-template-hash: 6b98b5cfbb
      spec:
        automountServiceAccountToken: true
        containers:
        - image: registry.k8s.io/defaultbackend-amd64:1.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: ingress-nginx-default-backend
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 50m
              memory: 50Mi
            requests:
              cpu: 10m
              memory: 20Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx-backend
        serviceAccountName: ingress-nginx-backend
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-13T19:53:34Z"
    generation: 2
    labels:
      k8s-app: kube-dns
      pod-template-hash: 98c69cbf4
    name: coredns-98c69cbf4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: c699e372-1dd5-4ae3-97d7-e93f0eed443c
    resourceVersion: "259207"
    uid: f28369b1-5432-4238-87b5-18a83d0bb2f3
  spec:
    replicas: 3
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 98c69cbf4
    template:
      metadata:
        annotations:
          version_hash: "1960218013"
        labels:
          k8s-app: kube-dns
          pod-template-hash: 98c69cbf4
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: iad.ocir.io/id9y6mi8tcky/oke-public-coredns@sha256:e32e8482ef16dbfd86896ece95e81111d5cb110811a65c3ce85df0ce2b69ca17
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - key: oci.oraclecloud.com/oke-is-preemptible
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 3
    fullyLabeledReplicas: 3
    observedGeneration: 2
    readyReplicas: 3
    replicas: 3
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-13T19:53:36Z"
    generation: 1
    labels:
      k8s-app: kube-dns-autoscaler
      pod-template-hash: 6d5986cc55
    name: kube-dns-autoscaler-6d5986cc55
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns-autoscaler
      uid: 9660865c-239d-45a7-bdf2-7b003e96df17
    resourceVersion: "259103"
    uid: a5b0d573-e9e8-4ccd-8082-58f36304b824
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns-autoscaler
        pod-template-hash: 6d5986cc55
    template:
      metadata:
        annotations:
          version_hash: "1960218013"
        labels:
          k8s-app: kube-dns-autoscaler
          pod-template-hash: 6d5986cc55
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=kube-dns-autoscaler
          - --target=deployment/coredns
          - --logtostderr=true
          - --v=2
          image: iad.ocir.io/id9y6mi8tcky/oke-public-cluster-proportional-autoscaler-amd64@sha256:1908914e0c9055edd754a633de2a37fd6811a64565317f2f44bf4adea85f0654
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: dns-autoscaler
        serviceAccountName: dns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: oci.oraclecloud.com/oke-is-preemptible
          operator: Exists
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:49:35Z"
    generation: 2
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      pod-template-hash: 555c567795
    name: loki-gateway-555c567795
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: loki-gateway
      uid: d1a3698c-7e52-46a7-a0bc-d8ee653a30ae
    resourceVersion: "1627064"
    uid: d6454f34-61bf-40be-9ebf-4a3de7164e80
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: gateway
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
        pod-template-hash: 555c567795
    template:
      metadata:
        annotations:
          checksum/config: 693f017d8081ff997c0d8cfa859fa719cc2cc2859d07f0b01b66cf21f88e6cf0
        labels:
          app.kubernetes.io/component: gateway
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          pod-template-hash: 555c567795
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: gateway
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        containers:
        - image: docker.io/nginxinc/nginx-unprivileged:1.29-alpine
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 8080
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: config
          - mountPath: /tmp
            name: tmp
          - mountPath: /docker-entrypoint.d
            name: docker-entrypoint-d-override
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 101
          runAsGroup: 101
          runAsNonRoot: true
          runAsUser: 101
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: loki-gateway
          name: config
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: docker-entrypoint-d-override
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-18T00:30:37Z"
    generation: 1
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      pod-template-hash: 697968b797
    name: loki-gateway-697968b797
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: loki-gateway
      uid: d1a3698c-7e52-46a7-a0bc-d8ee653a30ae
    resourceVersion: "1627053"
    uid: c3b049c8-dc12-466d-9bec-16c56913b251
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: gateway
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
        pod-template-hash: 697968b797
    template:
      metadata:
        annotations:
          checksum/config: ff015bd125f529abec13dae9359150ba80dd9fa80ef5698465aceb2b78b27aed
        labels:
          app.kubernetes.io/component: gateway
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          pod-template-hash: 697968b797
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: gateway
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        containers:
        - image: docker.io/nginxinc/nginx-unprivileged:1.29-alpine
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 8080
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: config
          - mountPath: /tmp
            name: tmp
          - mountPath: /docker-entrypoint.d
            name: docker-entrypoint-d-override
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 101
          runAsGroup: 101
          runAsNonRoot: true
          runAsUser: 101
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: loki-gateway
          name: config
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: docker-entrypoint-d-override
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
      deployment.kubernetes.io/revision-history: "5"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-20T19:09:01Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
      pod-template-hash: 58b9b5c88
    name: prometheus-grafana-58b9b5c88
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-grafana
      uid: b7c75b5d-7e74-4ae2-ab70-dadf3ec0410f
    resourceVersion: "2773212"
    uid: 7c8918f3-6915-4d91-8467-dd08b1ee07c2
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
        pod-template-hash: 58b9b5c88
    template:
      metadata:
        annotations:
          checksum/config: 5e2dee057f387b2e4a95327db5751d7cc7fa548be7adf08c4bf0d031d3da58e5
          checksum/dashboards-json-config: 9221cc5f8060f7a4e997fce086c9d7bcc5f450c0700f4044b253fa0944e9d5ee
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 116bc63539f0196c60512a28c581aef2b46733356fb67f51bf2ca62b10671bf8
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-11-18T01:14:01Z"
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.2.1
          helm.sh/chart: grafana-10.1.4
          pod-template-hash: 58b9b5c88
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          image: docker.io/grafana/grafana:12.2.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
            name: config
            subPath: dashboardproviders.yaml
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        - args:
          - -c
          - mkdir -p /var/lib/grafana/dashboards/default && /bin/sh -x /etc/grafana/download_dashboards.sh
          command:
          - /bin/sh
          image: docker.io/curlimages/curl:8.9.1
          imagePullPolicy: IfNotPresent
          name: download-dashboards
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/download_dashboards.sh
            name: config
            subPath: download_dashboards.sh
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-dashboards-default
          name: dashboards-default
        - name: storage
          persistentVolumeClaim:
            claimName: prometheus-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      deployment.kubernetes.io/revision-history: "4"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-18T01:14:00Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
      pod-template-hash: 6bbf76b649
    name: prometheus-grafana-6bbf76b649
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-grafana
      uid: b7c75b5d-7e74-4ae2-ab70-dadf3ec0410f
    resourceVersion: "2773221"
    uid: 0cd0e757-d637-45a6-97ca-03f09efca123
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
        pod-template-hash: 6bbf76b649
    template:
      metadata:
        annotations:
          checksum/config: 52c069bd5935ee5c739c82b1f2698c25d3e3d93ee9e70627a51f8349f8360365
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 116bc63539f0196c60512a28c581aef2b46733356fb67f51bf2ca62b10671bf8
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-11-18T01:14:01Z"
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.2.1
          helm.sh/chart: grafana-10.1.4
          pod-template-hash: 6bbf76b649
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          image: docker.io/grafana/grafana:12.2.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
            name: config
            subPath: dashboardproviders.yaml
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: prometheus-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T23:58:30Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
      pod-template-hash: 6f7fb459bf
    name: prometheus-grafana-6f7fb459bf
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-grafana
      uid: b7c75b5d-7e74-4ae2-ab70-dadf3ec0410f
    resourceVersion: "1637232"
    uid: 91468f69-de0e-42f9-83e4-28e7ae69ba01
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
        pod-template-hash: 6f7fb459bf
    template:
      metadata:
        annotations:
          checksum/config: 52c069bd5935ee5c739c82b1f2698c25d3e3d93ee9e70627a51f8349f8360365
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 116bc63539f0196c60512a28c581aef2b46733356fb67f51bf2ca62b10671bf8
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-11-17T23:58:31Z"
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.2.1
          helm.sh/chart: grafana-10.1.4
          pod-template-hash: 6f7fb459bf
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          image: docker.io/grafana/grafana:12.2.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
            name: config
            subPath: dashboardproviders.yaml
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: prometheus-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
      pod-template-hash: "7896896894"
    name: prometheus-grafana-7896896894
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-grafana
      uid: b7c75b5d-7e74-4ae2-ab70-dadf3ec0410f
    resourceVersion: "1619713"
    uid: 088bab9a-3737-41c2-99db-e8a573d2f443
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
        pod-template-hash: "7896896894"
    template:
      metadata:
        annotations:
          checksum/config: 52c069bd5935ee5c739c82b1f2698c25d3e3d93ee9e70627a51f8349f8360365
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 116bc63539f0196c60512a28c581aef2b46733356fb67f51bf2ca62b10671bf8
          kubectl.kubernetes.io/default-container: grafana
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.2.1
          helm.sh/chart: grafana-10.1.4
          pod-template-hash: "7896896894"
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          image: docker.io/grafana/grafana:12.2.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
            name: config
            subPath: dashboardproviders.yaml
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: prometheus-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-18T01:01:28Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.2.1
      helm.sh/chart: grafana-10.1.4
      pod-template-hash: 8489bd989c
    name: prometheus-grafana-8489bd989c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-grafana
      uid: b7c75b5d-7e74-4ae2-ab70-dadf3ec0410f
    resourceVersion: "1640061"
    uid: 2214c3ff-ef5c-4872-8155-99a657a81e88
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: grafana
        pod-template-hash: 8489bd989c
    template:
      metadata:
        annotations:
          checksum/config: 52c069bd5935ee5c739c82b1f2698c25d3e3d93ee9e70627a51f8349f8360365
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 116bc63539f0196c60512a28c581aef2b46733356fb67f51bf2ca62b10671bf8
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2025-11-18T01:01:29Z"
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.2.1
          helm.sh/chart: grafana-10.1.4
          pod-template-hash: 8489bd989c
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          image: docker.io/grafana/grafana:12.2.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
            name: config
            subPath: dashboardproviders.yaml
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: prometheus-grafana
        serviceAccountName: prometheus-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: prometheus-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-6.4.1
      pod-template-hash: 69465cd5f
      release: prometheus
    name: prometheus-kube-state-metrics-69465cd5f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: ab44d8c3-d780-4d66-ba94-c2ccd61ba15c
    resourceVersion: "1599744"
    uid: 4383324c-e459-4dd5-a839-fed43a4769fe
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 69465cd5f
    template:
      metadata:
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.17.0
          helm.sh/chart: kube-state-metrics-6.4.1
          pod-template-hash: 69465cd5f
          release: prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:52:41Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      pod-template-hash: 5cf786bfc5
      release: prometheus
    name: prometheus-operator-5cf786bfc5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-operator
      uid: 71bf0d38-3bcb-4b57-b167-a0ecdde45642
    resourceVersion: "1599558"
    uid: 96dd10ef-5180-454f-a68d-144e5517b0ff
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 5cf786bfc5
        release: prometheus
    template:
      metadata:
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 79.5.0
          chart: kube-prometheus-stack-79.5.0
          heritage: Helm
          pod-template-hash: 5cf786bfc5
          release: prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/prometheus-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.86.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-operator
        serviceAccountName: prometheus-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: prometheus-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "16532626029758763096"
    creationTimestamp: "2025-11-17T22:52:49Z"
    generation: 1
    labels:
      alertmanager: prometheus-alertmanager
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: prometheus-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      managed-by: prometheus-operator
      release: prometheus
    name: alertmanager-prometheus-alertmanager
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: prometheus-alertmanager
      uid: fd82508a-d26f-406a-a528-17b855efb7c9
    resourceVersion: "1599959"
    uid: 271bddf4-a63a-4945-b302-bb774ed3e64d
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: prometheus-alertmanager
        app.kubernetes.io/instance: prometheus-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
        labels:
          alertmanager: prometheus-alertmanager
          app.kubernetes.io/instance: prometheus-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.29.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - alertmanager
                  - key: alertmanager
                    operator: In
                    values:
                    - prometheus-alertmanager
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=120h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=http://prometheus-alertmanager.monitoring:9093
          - --web.route-prefix=/
          - --cluster.label=monitoring/prometheus-alertmanager
          - --cluster.peer=alertmanager-prometheus-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.29.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-prometheus-alertmanager-db
            subPath: alertmanager-db
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
            name: cluster-tls-config
            readOnly: true
            subPath: cluster-tls-config.yaml
        - args:
          - --listen-address=:8080
          - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
          - --reload-url=http://127.0.0.1:9093/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-init
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-alertmanager
        serviceAccountName: prometheus-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-prometheus-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-prometheus-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-prometheus-alertmanager-web-config
        - name: cluster-tls-config
          secret:
            defaultMode: 420
            secretName: alertmanager-prometheus-alertmanager-cluster-tls-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: alertmanager-prometheus-alertmanager-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
        storageClassName: oci-bv
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-prometheus-alertmanager-6879d8bdb5
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-prometheus-alertmanager-6879d8bdb5
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-18T00:30:37Z"
    generation: 3
    labels:
      app.kubernetes.io/component: single-binary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      app.kubernetes.io/version: 3.5.7
      helm.sh/chart: loki-6.46.0
    name: loki
    namespace: monitoring
    resourceVersion: "1632554"
    uid: 81bae65b-a59e-41e7-bead-f793480adf30
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Delete
      whenScaled: Delete
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: single-binary
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    serviceName: loki-headless
    template:
      metadata:
        annotations:
          checksum/config: 90405c87629d83f9f75396f01cab18a6713e2ea1df9996b2bff192be91221389
          kubectl.kubernetes.io/default-container: loki
          storage/size: 20Gi
        labels:
          app.kubernetes.io/component: single-binary
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          app.kubernetes.io/part-of: memberlist
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: single-binary
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/etc/loki/config/config.yaml
          - -target=all
          image: docker.io/grafana/loki:3.5.7
          imagePullPolicy: IfNotPresent
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: http-memberlist
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1536Mi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /etc/loki/config
            name: config
          - mountPath: /etc/loki/runtime-config
            name: runtime-config
          - mountPath: /var/loki
            name: storage
          - mountPath: /rules
            name: sc-rules-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: loki_rule
          - name: FOLDER
            value: /rules
          - name: RESOURCE
            value: both
          - name: WATCH_SERVER_TIMEOUT
            value: "60"
          - name: WATCH_CLIENT_TIMEOUT
            value: "60"
          - name: LOG_LEVEL
            value: INFO
          image: docker.io/kiwigrid/k8s-sidecar:1.30.10
          imagePullPolicy: IfNotPresent
          name: loki-sc-rules
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rules
            name: sc-rules-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: loki
          name: config
        - configMap:
            defaultMode: 420
            name: loki-runtime
          name: runtime-config
        - emptyDir: {}
          name: sc-rules-volume
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
        storageClassName: oci-bv
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: loki-7d654cb9d4
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updateRevision: loki-7d654cb9d4
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "10657755776000064183"
    creationTimestamp: "2025-11-17T22:52:49Z"
    generation: 4
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: prometheus-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 79.5.0
      chart: kube-prometheus-stack-79.5.0
      heritage: Helm
      managed-by: prometheus-operator
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: prometheus-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: prometheus-prometheus
      release: prometheus
    name: prometheus-prometheus-prometheus
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: prometheus-prometheus
      uid: 4792f7c1-b96a-4c08-b06b-8805e3bb261b
    resourceVersion: "2769756"
    uid: 92bc4520-0873-4275-b562-18b00715ee2a
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: prometheus-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: prometheus-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
        labels:
          app.kubernetes.io/instance: prometheus-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 3.7.3
          operator.prometheus.io/name: prometheus-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: prometheus-prometheus
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - prometheus
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                    - prometheus-prometheus
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.enable-remote-write-receiver
          - --web.external-url=http://prometheus-prometheus.monitoring:9090
          - --web.route-prefix=/
          - --storage.tsdb.retention.time=15d
          - --storage.tsdb.retention.size=18GB
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v3.7.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-prometheus-prometheus-db
            subPath: prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
            name: prometheus-prometheus-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
            name: prometheus-prometheus-prometheus-rulefiles-0
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.86.2
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-init
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-prometheus-prometheus-rulefiles-0
            name: prometheus-prometheus-prometheus-rulefiles-0
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-prometheus
        serviceAccountName: prometheus-prometheus
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-prometheus-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-prometheus-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-prometheus-prometheus-rulefiles-0
          name: prometheus-prometheus-prometheus-rulefiles-0
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-prometheus-prometheus-web-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: prometheus-prometheus-prometheus-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
        storageClassName: oci-bv
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-prometheus-prometheus-c569bdbd
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-prometheus-prometheus-c569bdbd
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-11-17T22:56:16Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.9.0
      helm.sh/chart: tempo-1.24.0
    name: tempo
    namespace: monitoring
    resourceVersion: "1601140"
    uid: d2420b0b-014a-4ca7-bf37-8c5447ed6a42
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: tempo
        app.kubernetes.io/name: tempo
    serviceName: tempo-headless
    template:
      metadata:
        annotations:
          checksum/config: 27e8162c5b1eab4bb959f8c48a5b0999ad352a32609ef4d0c5ff7500cd6d89d0
        labels:
          app.kubernetes.io/instance: tempo
          app.kubernetes.io/name: tempo
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/conf/tempo.yaml
          - -mem-ballast-size-mbs=1024
          image: grafana/tempo:2.9.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3200
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: tempo
          ports:
          - containerPort: 3200
            name: prom-metrics
            protocol: TCP
          - containerPort: 6831
            name: jaeger-thrift-c
            protocol: UDP
          - containerPort: 6832
            name: jaeger-thrift-b
            protocol: UDP
          - containerPort: 14268
            name: jaeger-thrift-h
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55680
            name: otlp-legacy
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 55681
            name: otlp-httplegacy
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 55678
            name: opencensus
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3200
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: tempo-conf
          - mountPath: /var/tempo
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: tempo
        serviceAccountName: tempo
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: tempo
          name: tempo-conf
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: oci-bv
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: tempo-845fcbc677
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: tempo-845fcbc677
    updatedReplicas: 1
kind: List
metadata:
  resourceVersion: ""
