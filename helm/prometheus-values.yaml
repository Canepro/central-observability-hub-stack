# Prometheus Standalone Helm Chart Values
# Chart: prometheus-community/prometheus
# Version: 27.47.0+

# --- ðŸš¨ CRITICAL: TOP LEVEL CONFIGURATION ðŸš¨ ---
# serverFiles must be at the top level, NOT nested under 'server:'
serverFiles:
  alerting_rules.yml:
    groups:
      - name: WorldTreeAlerts
        rules:
          # 1. Spoke Heartbeat: Kind Cluster (Training)
          # Logic: If the 'up' metric for the Kind cluster job is 0 for 2m, the proxy or lab is down.
          - alert: KindSpokeOffline
            expr: up{job="kind-kind-cluster"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Kind Spoke Offline"
              description: "The L4 proxy on port 6443 or the Lab Server is unreachable."

          # 2. Hub Infrastructure: OKE Storage
          # Logic: Alert when PVCs or Node volumes have < 15% space remaining.
          - alert: OKEStorageWarning
            expr: (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) * 100 < 15
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Low Disk Space on OKE"
              description: "OKE Storage on {{ $labels.node }} is below 15% free space."

          # 3. GitOps Heartbeat: Argo CD Controller
          # Logic: Ensure the metrics service enabled via Terraform is being scraped.
          - alert: ArgoCDControllerDown
            expr: up{job="argocd-application-controller-metrics"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "ArgoCD Monitoring Down"
              description: "The ArgoCD application-controller is not reporting metrics."

          # 4. GitOps Health: Application Status
          # Logic: Alerts if any Argo CD application enters a 'Degraded' state (Sync failure, CrashLoop, etc).
          - alert: ArgoCDApplicationDegraded
            expr: argocd_app_info{health_status="Degraded"} == 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "GitOps Application Degraded"
              description: "Application {{ $labels.name }} in project {{ $labels.project }} is in a failed state."

# Add kubelet volume stats so Grafana can show live PVC usage (% full).
extraScrapeConfigs: |
  - job_name: kubelet-volume-stats
    scheme: https
    kubernetes_sd_configs:
      - role: node
    tls_config:
      insecure_skip_verify: true
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics
    metric_relabel_configs:
      - source_labels: [__name__]
        action: keep
        regex: kubelet_volume_stats_(used_bytes|capacity_bytes|available_bytes|inodes|inodes_free)

configmapReload:
  prometheus:
    # sidecar to reload config when configmap changes
    extraArgs:
      listen-address: 0.0.0.0:8080

server:
  # Retention setting for historical metrics
  retention: "15d"

  # Lifecycle flags allow us to reload config via HTTP POST /-/reload
  extraFlags:
    - web.enable-remote-write-receiver
    - web.enable-lifecycle

  # Pod annotations to force restart when config changes
  # Update the timestamp to trigger a rolling restart via GitOps
  podAnnotations:
    config-reload-timestamp: "2026-01-18T19:09:00Z"

  # Storage configuration for OKE (Oracle Block Volume)
  persistentVolume:
    enabled: true
    size: 50Gi
    storageClass: oci-bv
    accessModes:
      - ReadWriteOnce

  # Resource limits for Hub cluster stability
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 2
      memory: 2Gi

  # Service configuration
  service:
    servicePort: 80
    type: ClusterIP

  # Global Scrape Intervals and External Labels
  global:
    scrape_interval: 30s
    evaluation_interval: 30s
    # External labels: Added to all metrics scraped by this Prometheus server
    # This identifies metrics from the OKE Hub cluster itself
    external_labels:
      cluster: oke-hub
      environment: production
      workspace: hub
      domain: observability.canepro.me

# Alertmanager configuration for Multi-Cluster SMTP Notifications
alertmanager:
  enabled: true

  # Lab-friendly persistence and resource settings
  persistence:
    enabled: false
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi

  # Securely inject SMTP credentials from your K8s Secret
  extraEnvVars:
    - name: SMTP_PASSWORD
      valueFrom:
        secretKeyRef:
          name: grafana-smtp-credentials
          key: password
    - name: SMTP_USER
      valueFrom:
        secretKeyRef:
          name: grafana-smtp-credentials
          key: user
    - name: SMTP_FROM
      valueFrom:
        secretKeyRef:
          name: grafana-smtp-credentials
          key: from_address
    - name: SMTP_TO
      valueFrom:
        secretKeyRef:
          name: grafana-smtp-credentials
          key: to_address

  config:
    global:
      resolve_timeout: 5m
      # Variables injected via extraEnvVars
      smtp_from: '${SMTP_FROM}'
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_auth_username: '${SMTP_USER}'
      smtp_auth_password: '${SMTP_PASSWORD}'
      smtp_require_tls: true

    route:
      group_by: ['alertname', 'cluster', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'gmail-notifications'

    receivers:
    - name: 'gmail-notifications'
      email_configs:
      - to: '${SMTP_TO}'
        send_resolved: true # Send an email when the issue is cleared

# Node Exporter for Infrastructure Metrics
nodeExporter:
  enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 128Mi

# Kube State Metrics for Cluster Objects
kubeStateMetrics:
  enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

# Pushgateway for ephemeral/batch jobs
pushgateway:
  enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi
